{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "\n",
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Unfreeze more layers since we have substantial data\n",
    "        for param in self.roberta.encoder.layer[-8:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Shared features layer\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use mean pooling instead of just [CLS] token\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Get shared features\n",
    "        shared_features = self.shared_features(pooled_output)\n",
    "        \n",
    "        # Get task-specific predictions\n",
    "        emotional_logits = self.emotional_classifier(shared_features)\n",
    "        political_logits = self.political_classifier(shared_features)\n",
    "        \n",
    "        return emotional_logits, political_logits\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"Load the trained model\"\"\"\n",
    "    model_state = torch.load(model_path, map_location=device)\n",
    "    model = PoliticalSpeechClassifier()\n",
    "    model.load_state_dict(model_state['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_speech(model, tokenizer, speech_text, device):\n",
    "    \"\"\"Make predictions for a single speech\"\"\"\n",
    "    encoding = tokenizer(\n",
    "        speech_text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        emotional_probs = torch.softmax(emotional_logits, dim=1)\n",
    "        political_probs = torch.softmax(political_logits, dim=1)\n",
    "        \n",
    "        emotional_pred = torch.argmax(emotional_probs, dim=1).item() + 1\n",
    "        political_pred = torch.argmax(political_probs, dim=1).item() + 1\n",
    "        \n",
    "        emotional_confidence = emotional_probs[0][emotional_pred-1].item()\n",
    "        political_confidence = political_probs[0][political_pred-1].item()\n",
    "    \n",
    "    return {\n",
    "        'emotional_intensity': emotional_pred,\n",
    "        'emotional_confidence': emotional_confidence,\n",
    "        'political_spectrum': political_pred,\n",
    "        'political_confidence': political_confidence\n",
    "    }\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    \"\"\"Calculate various accuracy metrics\"\"\"\n",
    "    correct = sum(1 for t, p in zip(true_values, predicted_values) if t == p)\n",
    "    accuracy = correct / len(true_values)\n",
    "    \n",
    "    # Calculate how far off predictions are\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    \n",
    "    # Calculate percentage of predictions that are off by at most 1\n",
    "    off_by_one = sum(1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 1)\n",
    "    off_by_one_pct = off_by_one / len(true_values)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'off_by_one_pct': off_by_one_pct\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(true_values, predicted_values, title, save_path):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(true_values, predicted_values)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prediction_distribution(predictions, title, save_path):\n",
    "    \"\"\"Plot distribution of predictions\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=predictions)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "\n",
    "def evaluate_model(json_path, model_path, n_samples=100, save_dir='evaluation_results'):\n",
    "    \"\"\"Main evaluation function\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Randomly sample n speeches\n",
    "    speech_ids = random.sample(list(data.keys()), n_samples)\n",
    "    sampled_data = {k: data[k] for k in speech_ids}\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(model_path, device)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = []\n",
    "    true_emotional = []\n",
    "    true_political = []\n",
    "    pred_emotional = []\n",
    "    pred_political = []\n",
    "    \n",
    "    for speech_id, speech_data in tqdm(sampled_data.items()):\n",
    "        # Get true values\n",
    "        true_emotional.append(speech_data['emotional_intensity'])\n",
    "        true_political.append(speech_data['political_spectrum'])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = predict_speech(model, tokenizer, speech_data['speech'], device)\n",
    "        predictions.append(pred)\n",
    "        pred_emotional.append(pred['emotional_intensity'])\n",
    "        pred_political.append(pred['political_spectrum'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    emotional_metrics = calculate_metrics(true_emotional, pred_emotional)\n",
    "    political_metrics = calculate_metrics(true_political, pred_political)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'emotional_intensity': emotional_metrics,\n",
    "        'political_spectrum': political_metrics\n",
    "    }\n",
    "    \n",
    "    with open(f\"{save_dir}/evaluation_metrics.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix(\n",
    "        true_emotional, \n",
    "        pred_emotional, \n",
    "        \"Emotional Intensity Confusion Matrix\",\n",
    "        f\"{save_dir}/emotional_confusion_matrix.png\"\n",
    "    )\n",
    "    \n",
    "    plot_confusion_matrix(\n",
    "        true_political, \n",
    "        pred_political, \n",
    "        \"Political Spectrum Confusion Matrix\",\n",
    "        f\"{save_dir}/political_confusion_matrix.png\"\n",
    "    )\n",
    "    \n",
    "    # Plot prediction distributions\n",
    "    plot_prediction_distribution(\n",
    "        pred_emotional,\n",
    "        \"Distribution of Emotional Intensity Predictions\",\n",
    "        f\"{save_dir}/emotional_distribution.png\"\n",
    "    )\n",
    "    \n",
    "    plot_prediction_distribution(\n",
    "        pred_political,\n",
    "        \"Distribution of Political Spectrum Predictions\",\n",
    "        f\"{save_dir}/political_distribution.png\"\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(\"\\nEmotional Intensity Metrics:\")\n",
    "    print(f\"Accuracy: {emotional_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Mean Absolute Error: {emotional_metrics['mae']:.2f}\")\n",
    "    print(f\"Predictions within ±1: {emotional_metrics['off_by_one_pct']:.2%}\")\n",
    "    \n",
    "    print(\"\\nPolitical Spectrum Metrics:\")\n",
    "    print(f\"Accuracy: {political_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Mean Absolute Error: {political_metrics['mae']:.2f}\")\n",
    "    print(f\"Predictions within ±1: {political_metrics['off_by_one_pct']:.2%}\")\n",
    "    \n",
    "    return metrics, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/xhv26dgs4tscxfcc4mdbh8zw0000gn/T/ipykernel_48146/351370901.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(model_path, map_location=device)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics...\n",
      "\n",
      "Evaluation Summary:\n",
      "\n",
      "Emotional Intensity Metrics:\n",
      "Accuracy: 82.80%\n",
      "Mean Absolute Error: 0.18\n",
      "Predictions within ±1: 99.60%\n",
      "\n",
      "Political Spectrum Metrics:\n",
      "Accuracy: 85.40%\n",
      "Mean Absolute Error: 0.15\n",
      "Predictions within ±1: 99.20%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set these paths according to your setup\n",
    "    JSON_PATH = \"speeches_111_gpt_axis_labels_copy.json\"  # Path to your JSON file\n",
    "    MODEL_PATH = \"model_artifacts_20241202_142615_copy/model.pt\"  # Path to your saved model\n",
    "    N_SAMPLES = 500  # Number of speeches to evaluate\n",
    "    \n",
    "    metrics, predictions = evaluate_model(\n",
    "        json_path=JSON_PATH,\n",
    "        model_path=MODEL_PATH,\n",
    "        n_samples=N_SAMPLES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
