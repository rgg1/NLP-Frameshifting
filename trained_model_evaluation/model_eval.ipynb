{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, f1_score, classification_report\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Unfreeze more layers since we have more data\n",
    "        for param in self.roberta.encoder.layer[-8:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Shared features layer\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use mean pooling instead of just [CLS] token\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Get shared features\n",
    "        shared_features = self.shared_features(pooled_output)\n",
    "        \n",
    "        # Get task-specific predictions\n",
    "        emotional_logits = self.emotional_classifier(shared_features)\n",
    "        political_logits = self.political_classifier(shared_features)\n",
    "        \n",
    "        return emotional_logits, political_logits\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Load the trained model\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): path to the trained model\n",
    "        device (str): 'cpu' or 'cuda'\n",
    "\n",
    "    Returns:\n",
    "        model: the trained model\n",
    "    \"\"\"\n",
    "    model_state = torch.load(model_path, map_location=device)\n",
    "    model = PoliticalSpeechClassifier()\n",
    "    model.load_state_dict(model_state['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_speech(model, tokenizer, speech_text, device):\n",
    "    \"\"\"\n",
    "    Make predictions for a single speech\n",
    "    \n",
    "    Args:\n",
    "        model: the trained model\n",
    "        tokenizer: the tokenizer used to preprocess the text\n",
    "        speech_text (str): the text of the speech\n",
    "        device (str): 'cpu' or 'cuda'\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary containing the predicted values\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        speech_text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        emotional_probs = torch.softmax(emotional_logits, dim=1)\n",
    "        political_probs = torch.softmax(political_logits, dim=1)\n",
    "        \n",
    "        emotional_pred = torch.argmax(emotional_probs, dim=1).item() + 1\n",
    "        political_pred = torch.argmax(political_probs, dim=1).item() + 1\n",
    "        \n",
    "        emotional_confidence = emotional_probs[0][emotional_pred-1].item()\n",
    "        political_confidence = political_probs[0][political_pred-1].item()\n",
    "    \n",
    "    return {\n",
    "        'emotional_intensity': emotional_pred,\n",
    "        'emotional_confidence': emotional_confidence,\n",
    "        'political_spectrum': political_pred,\n",
    "        'political_confidence': political_confidence\n",
    "    }\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Calculate various accuracy metrics\n",
    "    \n",
    "    Args:\n",
    "        true_values (list): list of true values\n",
    "        predicted_values (list): list of predicted values\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary containing the calculated metrics\n",
    "    \"\"\"\n",
    "    correct = sum(1 for t, p in zip(true_values, predicted_values) if t == p)\n",
    "    accuracy = correct / len(true_values)\n",
    "    \n",
    "    # Calculate how far off predictions are\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    \n",
    "    # Calculate percentage of predictions that are off by at most 1\n",
    "    off_by_one = sum(1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 1)\n",
    "    off_by_one_pct = off_by_one / len(true_values)\n",
    "\n",
    "    # Micro F1\n",
    "    f1_micro = f1_score(true_values, predicted_values, average='micro')\n",
    "    # Macro F1\n",
    "    f1_macro = f1_score(true_values, predicted_values, average='macro')\n",
    "    # Weighted F1\n",
    "    f1_weighted = f1_score(true_values, predicted_values, average='weighted')\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(true_values, predicted_values, \n",
    "                                       labels=[1, 2, 3, 4, 5],\n",
    "                                       output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'off_by_one_pct': off_by_one_pct,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'class_report': class_report\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_values, predicted_values, title, save_path):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(true_values, predicted_values)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prediction_distribution(predictions, title, save_path):\n",
    "    \"\"\"Plot distribution of predictions\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=predictions)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(json_path, model_path, n_samples=100, save_dir='evaluation_results'):\n",
    "    \"\"\"\n",
    "    Main evaluation function\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): path to the JSON file containing the speech data\n",
    "        model_path (str): path to the trained model\n",
    "        n_samples (int): number of speeches to sample\n",
    "        save_dir (str): directory to save evaluation results\n",
    "\n",
    "    Returns:\n",
    "        tuple(dict, list): a dictionary containing the calculated metrics, and\n",
    "            a list of dictionaries containing the predicted values\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Randomly sample n speeches\n",
    "    speech_ids = random.sample(list(data.keys()), n_samples)\n",
    "    sampled_data = {k: data[k] for k in speech_ids}\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(model_path, device)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = []\n",
    "    true_emotional = []\n",
    "    true_political = []\n",
    "    pred_emotional = []\n",
    "    pred_political = []\n",
    "    \n",
    "    for speech_id, speech_data in tqdm(sampled_data.items()):\n",
    "        # Get true values\n",
    "        true_emotional.append(speech_data['emotional_intensity'])\n",
    "        true_political.append(speech_data['political_spectrum'])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = predict_speech(model, tokenizer, speech_data['speech'], device)\n",
    "        predictions.append(pred)\n",
    "        pred_emotional.append(pred['emotional_intensity'])\n",
    "        pred_political.append(pred['political_spectrum'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    emotional_metrics = calculate_metrics(true_emotional, pred_emotional)\n",
    "    political_metrics = calculate_metrics(true_political, pred_political)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'emotional_intensity': emotional_metrics,\n",
    "        'political_spectrum': political_metrics\n",
    "    }\n",
    "    \n",
    "    with open(f\"{save_dir}/evaluation_metrics.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix(\n",
    "        true_emotional, \n",
    "        pred_emotional, \n",
    "        \"Emotional Intensity Confusion Matrix\",\n",
    "        f\"{save_dir}/emotional_confusion_matrix.png\"\n",
    "    )\n",
    "    \n",
    "    plot_confusion_matrix(\n",
    "        true_political, \n",
    "        pred_political, \n",
    "        \"Political Spectrum Confusion Matrix\",\n",
    "        f\"{save_dir}/political_confusion_matrix.png\"\n",
    "    )\n",
    "    \n",
    "    # Plot prediction distributions\n",
    "    plot_prediction_distribution(\n",
    "        pred_emotional,\n",
    "        \"Distribution of Emotional Intensity Predictions\",\n",
    "        f\"{save_dir}/emotional_distribution.png\"\n",
    "    )\n",
    "    \n",
    "    plot_prediction_distribution(\n",
    "        pred_political,\n",
    "        \"Distribution of Political Spectrum Predictions\",\n",
    "        f\"{save_dir}/political_distribution.png\"\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(\"\\nEmotional Intensity Metrics:\")\n",
    "    print(f\"Accuracy: {emotional_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Mean Absolute Error: {emotional_metrics['mae']:.2f}\")\n",
    "    print(f\"Predictions within ±1: {emotional_metrics['off_by_one_pct']:.2%}\")\n",
    "    print(f\"F1 Score (micro): {emotional_metrics['f1_micro']:.2f}\")\n",
    "    print(f\"F1 Score (macro): {emotional_metrics['f1_macro']:.2f}\")\n",
    "    print(f\"F1 Score (weighted): {emotional_metrics['f1_weighted']:.2f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    for score in range(1, 6):\n",
    "        metrics = emotional_metrics['class_report'][str(score)]\n",
    "        print(f\"\\nScore {score}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"  F1-score: {metrics['f1-score']:.2f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    print(\"\\nPolitical Spectrum Metrics:\")\n",
    "    print(f\"Accuracy: {political_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Mean Absolute Error: {political_metrics['mae']:.2f}\")\n",
    "    print(f\"Predictions within ±1: {political_metrics['off_by_one_pct']:.2%}\")\n",
    "    print(f\"F1 Score (micro): {political_metrics['f1_micro']:.2f}\")\n",
    "    print(f\"F1 Score (macro): {political_metrics['f1_macro']:.2f}\")\n",
    "    print(f\"F1 Score (weighted): {political_metrics['f1_weighted']:.2f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Political Classification Report:\")\n",
    "    for score in range(1, 6):\n",
    "        metrics = political_metrics['class_report'][str(score)]\n",
    "        print(f\"\\nScore {score}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"  F1-score: {metrics['f1-score']:.2f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "    \n",
    "    return metrics, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/xhv26dgs4tscxfcc4mdbh8zw0000gn/T/ipykernel_75615/2993411801.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(model_path, map_location=device)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:02<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics...\n",
      "\n",
      "Evaluation Summary:\n",
      "\n",
      "Emotional Intensity Metrics:\n",
      "Accuracy: 82.80%\n",
      "Mean Absolute Error: 0.18\n",
      "Predictions within ±1: 99.60%\n",
      "F1 Score (micro): 0.83\n",
      "F1 Score (macro): 0.85\n",
      "F1 Score (weighted): 0.83\n",
      "\n",
      "Detailed Classification Report:\n",
      "\n",
      "Score 1:\n",
      "  Precision: 0.90\n",
      "  Recall: 0.95\n",
      "  F1-score: 0.92\n",
      "  Support: 119.0\n",
      "\n",
      "Score 2:\n",
      "  Precision: 0.73\n",
      "  Recall: 0.67\n",
      "  F1-score: 0.70\n",
      "  Support: 95.0\n",
      "\n",
      "Score 3:\n",
      "  Precision: 0.85\n",
      "  Recall: 0.75\n",
      "  F1-score: 0.80\n",
      "  Support: 172.0\n",
      "\n",
      "Score 4:\n",
      "  Precision: 0.79\n",
      "  Recall: 0.95\n",
      "  F1-score: 0.86\n",
      "  Support: 101.0\n",
      "\n",
      "Score 5:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.92\n",
      "  F1-score: 0.96\n",
      "  Support: 13.0\n",
      "\n",
      "Political Spectrum Metrics:\n",
      "Accuracy: 85.40%\n",
      "Mean Absolute Error: 0.15\n",
      "Predictions within ±1: 99.20%\n",
      "F1 Score (micro): 0.85\n",
      "F1 Score (macro): 0.84\n",
      "F1 Score (weighted): 0.86\n",
      "\n",
      "Detailed Political Classification Report:\n",
      "\n",
      "Score 1:\n",
      "  Precision: 0.71\n",
      "  Recall: 0.91\n",
      "  F1-score: 0.80\n",
      "  Support: 11.0\n",
      "\n",
      "Score 2:\n",
      "  Precision: 0.71\n",
      "  Recall: 0.82\n",
      "  F1-score: 0.76\n",
      "  Support: 120.0\n",
      "\n",
      "Score 3:\n",
      "  Precision: 0.93\n",
      "  Recall: 0.86\n",
      "  F1-score: 0.89\n",
      "  Support: 299.0\n",
      "\n",
      "Score 4:\n",
      "  Precision: 0.84\n",
      "  Recall: 0.89\n",
      "  F1-score: 0.87\n",
      "  Support: 47.0\n",
      "\n",
      "Score 5:\n",
      "  Precision: 0.95\n",
      "  Recall: 0.83\n",
      "  F1-score: 0.88\n",
      "  Support: 23.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    JSON_PATH = \"speeches_111_gpt_axis_labels_copy.json\"\n",
    "    MODEL_PATH = \"../large-training-output/model_artifacts_20241202_142615/model.pt\"\n",
    "    N_SAMPLES = 500  # Number of speeches to evaluate\n",
    "    \n",
    "    metrics, predictions = evaluate_model(\n",
    "        json_path=JSON_PATH,\n",
    "        model_path=MODEL_PATH,\n",
    "        n_samples=N_SAMPLES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
