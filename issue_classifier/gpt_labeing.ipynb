{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member(BaseModel):\n",
    "    member_name: str\n",
    "    member_role: str\n",
    "    member_state: str\n",
    "\n",
    "class Staff(BaseModel):\n",
    "    staff_name: str\n",
    "    staff_role: str\n",
    "    staff_state: str\n",
    "\n",
    "class Subcommittee(BaseModel):\n",
    "    subcommittee_name: str\n",
    "    subcommittee_members: list[Union[Member, Staff]]\n",
    "\n",
    "class Committee(BaseModel):\n",
    "    committee_name: str\n",
    "    subcommittees: list[Subcommittee]\n",
    "\n",
    "class committees_json_schema(BaseModel):\n",
    "    committees: list[Committee]\n",
    "\n",
    "def extract_committee_info(text_chunk):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's API to extract committees, subcommittees, staff, member names, and roles from the given text.\n",
    "    Returns the information in JSON format as specified by the implementation.\n",
    "\n",
    "    Args:\n",
    "        text_chunk: str\n",
    "            Chunk of text to process\n",
    "    \n",
    "    Returns:\n",
    "        response: str\n",
    "            Extracted information in JSON format\n",
    "    \"\"\"\n",
    "    print(f\"Processing chunk of size: {len(text_chunk)} characters\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    Extract committees, subcommittees, member information, and staff information from this text into JSON format.\n",
    "                    Be thorough in extracting all relevant information (don't miss any names).\n",
    "                    For each committee:\n",
    "                    1. Find the committee name\n",
    "                    2. IMPORTANT: First process the main committee members and staff (all members and staff listed BEFORE any subcommittee section)\n",
    "                    - Create a subcommittee with the same name as the committee\n",
    "                    - Include all members and staff listed at the start of the committee section\n",
    "\n",
    "                    STAFF PROCESSING INSTRUCTIONS:\n",
    "                    - Look for major staff sections marked by 'STAFF', 'Majority Staff', 'Minority Staff', or similar headers\n",
    "                    - Process ALL staff hierarchically - Director level, Deputy level, Professional Staff, Administrative Staff, etc.\n",
    "                    - Pay special attention to indented staff listings which indicate reporting relationships\n",
    "                    - Look for staff listings in office-specific sections (e.g., \"Clerk's Office:\", \"Communications:\", etc.)\n",
    "                    - Process ALL contact information sections as they often contain additional staff listings\n",
    "                    - Watch for staff sections that continue across multiple pages\n",
    "\n",
    "                    For lines with two-column formats:\n",
    "                    * Process both the left and right sides of the line\n",
    "                    * Look for names separated by multiple spaces or tabs\n",
    "                    * Each side typically ends with a state and period\n",
    "\n",
    "                    For names that are split across lines with state information:\n",
    "                    * Check for entries where the state appears indented on the next line\n",
    "                    * Combine name and state information even when split by line breaks\n",
    "\n",
    "                    3. Then process any subcommittee section if it exists\n",
    "                    4. For committees with NO subcommittees, use the committee name as the subcommittee name\n",
    "                    5. Include everything until the next committee name appears\n",
    "                    6. After processing main sections, check for:\n",
    "                    - Additional staff listings at the end of committee sections\n",
    "                    - Staff listings in footnotes or supplementary sections\n",
    "                    - Professional staff members listed under special sections\n",
    "\n",
    "\n",
    "                    For each committee/subcommittee, record:\n",
    "                    - Members and their roles (Chair, Vice Chair, etc., use 'Member' if no explicit role listed)\n",
    "                    - States for members (use 'N/A' if no state listed)\n",
    "                    - Staff (names listed under 'STAFF' sections) and their roles (use 'Staff' if no explicit role listed)\n",
    "                    - States for staff (use 'N/A' if no state listed)\n",
    "\n",
    "                    Important details:\n",
    "                    - Include each name in every committee/subcommittee they appear in\n",
    "                    - Process BOTH columns when lines are formatted in two columns\n",
    "                    - Look for multiple names per line (separated by commas, periods, or large spaces)\n",
    "                    - Check if entries continue on next line\n",
    "                    - Keep line indentation in mind when grouping information\n",
    "                    - Remember that the main committee members and staff come BEFORE any subcommittee listings\n",
    "                    - For two-column layouts, process right column with same care as left column\n",
    "                    - DON'T FORGET TO INCLUDE THE STAFF, most committees/subcommittees have staff listed under 'STAFF' sections\n",
    "\n",
    "                    Output the results in the existing JSON structure provided.\n",
    "                    \"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text_chunk\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                'type': 'json_schema',\n",
    "                'json_schema': \n",
    "                    {\n",
    "                        \"name\":\"_\", \n",
    "                        \"schema\": committees_json_schema.model_json_schema()\n",
    "                    }\n",
    "            },  \n",
    "            temperature=0.3,\n",
    "            # max_tokens=10000,\n",
    "            timeout=600  # 10 minute timeout per chunk\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_committee_file(file_name, input_dir, output_dir):\n",
    "    # Read file content\n",
    "    file_path = os.path.join(input_dir, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        print(f\"File loaded. Size: {len(content)} characters\")\n",
    "    \n",
    "    print(\"Splitting content into chunks...\")\n",
    "    chunks = chunk_text(content)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    \n",
    "    all_committees = {\"committees\": []}\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk_num, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nProcessing chunk {chunk_num}/{len(chunks)}\")\n",
    "        try:\n",
    "            # Extract committee information in JSON format\n",
    "            committee_info = extract_committee_info(chunk)\n",
    "            \n",
    "            try:\n",
    "                # Parse the JSON response\n",
    "                chunk_data = json.loads(committee_info)\n",
    "                \n",
    "                # Merge committees from this chunk\n",
    "                if \"committees\" in chunk_data:\n",
    "                    all_committees[\"committees\"].extend(chunk_data[\"committees\"])\n",
    "                \n",
    "                print(f\"Successfully processed chunk {chunk_num}\")\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON Decode Error in chunk {chunk_num}: {str(e)}\")\n",
    "                # Save the problematic chunk\n",
    "                error_file_path = os.path.join(output_dir, f'{file_name}_chunk{chunk_num}_error.txt')\n",
    "                with open(error_file_path, 'w') as error_file:\n",
    "                    error_file.write(committee_info)\n",
    "                print(f\"Problematic chunk saved to {error_file_path}\")\n",
    "                continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save the combined results\n",
    "    json_file_path = os.path.join(output_dir, f'{file_name}_output.json') # output file name\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(all_committees, json_file, indent=2)\n",
    "    print(f\"\\nCombined results saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_committee_files(congress_number, committee_files):\n",
    "    \"\"\"\n",
    "    Process committee files for a given Congress number and type (Senate or House).\n",
    "    Extract committee information from each file and save the output in JSON format.\n",
    "\n",
    "    Args:\n",
    "        congress_number: int\n",
    "            Congress number to process\n",
    "        committee_files: dict\n",
    "            Dictionary containing committee file names for Senate or House\n",
    "    \"\"\"\n",
    "    # folder to save the output JSON files\n",
    "    output_dir = f\"outputs/{congress_number}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in tqdm(committee_files[f'congress_{congress_number}']):\n",
    "        input_dir = f'congressional_directory_files/congress_{congress_number}/txt'\n",
    "        process_committee_file(file_name, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../small_speech_data/speeches_113_trimmed.txt\n",
      "Found 6563 speeches\n",
      "Split into 1179 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   1%|          | 10/1179 [00:29<57:19,  2.94s/it]\n",
      "Processing files:  50%|█████     | 1/2 [00:29<00:29, 29.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 11\n",
      "\n",
      "Results saved to outputs/speeches_113_trimmed_analysis.json\n",
      "Processing file: ../small_speech_data/speeches_114_trimmed.txt\n",
      "Found 5456 speeches\n",
      "Split into 913 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   1%|          | 10/913 [00:25<37:59,  2.52s/it]\n",
      "Processing files: 100%|██████████| 2/2 [00:54<00:00, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 11\n",
      "\n",
      "Results saved to outputs/speeches_114_trimmed_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "class Speech(BaseModel):\n",
    "    speech_id: str\n",
    "    topics: List[str]\n",
    "    \n",
    "class Speeches(BaseModel):\n",
    "    speeches: List[Speech]\n",
    "\n",
    "def read_speech_file(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads a speech file and returns a dictionary mapping speech_ids to speeches.\n",
    "    Only includes speeches with more than 30 words.\n",
    "    \"\"\"\n",
    "    speeches = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip header line\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # Split on pipe character\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) == 2:\n",
    "                speech_id, speech = parts\n",
    "                # Only add speech if it has more than 30 words\n",
    "                if len(speech.split()) > 30:\n",
    "                    speeches[speech_id] = speech\n",
    "    return speeches\n",
    "\n",
    "def chunk_speeches(speeches: Dict[str, str], max_chunk_size: int = 15000) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits speeches into chunks while keeping individual speeches intact.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = {}\n",
    "    current_size = 0\n",
    "    \n",
    "    for speech_id, speech in speeches.items():\n",
    "        speech_size = len(speech)\n",
    "        \n",
    "        # If adding this speech would exceed max size and we already have speeches,\n",
    "        # start a new chunk\n",
    "        if current_size + speech_size > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = {}\n",
    "            current_size = 0\n",
    "            \n",
    "        current_chunk[speech_id] = speech\n",
    "        current_size += speech_size\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_speech_topics(speeches: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's API to extract topics from the given speeches.\n",
    "    \"\"\"\n",
    "    # Prepare the speeches for analysis\n",
    "    speeches_text = \"\\n\\n\".join([f\"Speech ID: {id}\\nContent: {text}\" for id, text in speeches.items()])\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    Analyze each speech and identify its relevant topics. Topics should be chosen from this fixed list:\n",
    "                    - Governance and Democracy\n",
    "                    - Economy and Jobs\n",
    "                    - Health and Social Services\n",
    "                    - Education and Innovation\n",
    "                    - Environment and Energy\n",
    "                    - Defense and Security\n",
    "                    - Immigration and Border Policy\n",
    "                    - Justice and Civil Rights\n",
    "                    - Infrastructure and Transportation\n",
    "                    - Budget and Fiscal Responsibility\n",
    "\n",
    "                    For each speech, assign one or more topics that best match its content.\n",
    "                    Output should be in JSON format containing a list of objects, each with a speech_id and its corresponding topics list.\n",
    "                    Be precise and thorough in topic assignment.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": speeches_text\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": \n",
    "                    {\n",
    "                        \"name\":\"_\", \n",
    "                        \"schema\": Speeches.model_json_schema()\n",
    "                    }\n",
    "            },\n",
    "            temperature=0.3,\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_speech_file(file_path: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process a speech file and save the results.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # Read speeches\n",
    "    speeches = read_speech_file(file_path)\n",
    "    print(f\"Found {len(speeches)} speeches\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = chunk_speeches(speeches)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Process each chunk and combine results\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        try:\n",
    "            # Get topics for speeches in this chunk\n",
    "            topics_json = extract_speech_topics(chunk)\n",
    "            topics_results = json.loads(topics_json)\n",
    "            \n",
    "            # Combine speech text with topics\n",
    "            for speech in topics_results[\"speeches\"]:\n",
    "                speech_id = speech[\"speech_id\"]\n",
    "                all_results[speech_id] = {\n",
    "                    \"speech\": speeches[speech_id],\n",
    "                    \"topics\": speech[\"topics\"]\n",
    "                }\n",
    "            \n",
    "            print(f\"Successfully processed chunk {i+1}\")\n",
    "            # break after 10 chunks\n",
    "            if i == 10:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path).replace('.txt', '_analysis.json'))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "def process_speech_files(speech_files: List[str], input_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process multiple speech files.\n",
    "    \"\"\"\n",
    "    for file_name in tqdm(speech_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        process_speech_file(file_path, output_dir)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"../small_speech_data\"\n",
    "    output_dir = \"outputs\"\n",
    "    speech_files = [f for f in os.listdir(input_dir) if f.startswith(\"speeches_\") and f.endswith(\".txt\")]\n",
    "    process_speech_files(speech_files, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 1014.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Frequencies:\n",
      "----------------------------------------\n",
      "Governance and Democracy          :    125\n",
      "Economy and Jobs                  :     43\n",
      "Justice and Civil Rights          :     36\n",
      "Budget and Fiscal Responsibility  :     30\n",
      "Health and Social Services        :     20\n",
      "Infrastructure and Transportation :      4\n",
      "Defense and Security              :      2\n",
      "Environment and Energy            :      1\n",
      "----------------------------------------\n",
      "Total topics mentioned: 261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_topic_frequencies(input_dir: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Analyze JSON files in the input directory and count topic frequencies.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing the speech analysis JSON files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping topics to their frequencies\n",
    "    \"\"\"\n",
    "    # Initialize counter for topics\n",
    "    topic_counter = Counter()\n",
    "    \n",
    "    # Get all JSON files in directory\n",
    "    json_files = [f for f in os.listdir(input_dir) if f.endswith('_analysis.json')]\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in tqdm(json_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Go through each speech\n",
    "            for speech_id, speech_data in data.items():\n",
    "                # Add each topic to our counter\n",
    "                topic_counter.update(speech_data['topics'])\n",
    "    \n",
    "    # Convert Counter to regular dictionary and sort by frequency\n",
    "    topic_frequencies = dict(sorted(topic_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return topic_frequencies\n",
    "\n",
    "def print_topic_statistics(topic_frequencies: Dict[str, int]):\n",
    "    \"\"\"\n",
    "    Print formatted statistics about topic frequencies.\n",
    "    \"\"\"\n",
    "    print(\"\\nTopic Frequencies:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find the longest topic name for formatting\n",
    "    max_topic_length = max(len(topic) for topic in topic_frequencies.keys())\n",
    "    \n",
    "    # Print each topic and its count\n",
    "    for topic, count in topic_frequencies.items():\n",
    "        print(f\"{topic:<{max_topic_length}} : {count:>6}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total topics mentioned: {sum(topic_frequencies.values())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"outputs\"\n",
    "    \n",
    "    # Analyze frequencies\n",
    "    topic_frequencies = analyze_topic_frequencies(input_dir)\n",
    "    \n",
    "    # Print results\n",
    "    print_topic_statistics(topic_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
