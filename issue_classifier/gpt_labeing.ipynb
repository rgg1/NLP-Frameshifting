{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech(BaseModel):\n",
    "    speech_id: str\n",
    "    topics: List[str]\n",
    "    \n",
    "class Speeches(BaseModel):\n",
    "    speeches: List[Speech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speech_file(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads a speech file and returns a dictionary mapping speech_ids to speeches.\n",
    "    Only includes speeches with more than 30 words.\n",
    "    \"\"\"\n",
    "    speeches = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip header line\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # Split on pipe character\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) == 2:\n",
    "                speech_id, speech = parts\n",
    "                # Only add speech if it has more than 30 -> 35 words\n",
    "                if len(speech.split()) > 35:\n",
    "                    speeches[speech_id] = speech\n",
    "    return speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_speeches(speeches: Dict[str, str], max_chunk_size: int = 20000) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits speeches into chunks while keeping individual speeches intact.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = {}\n",
    "    current_size = 0\n",
    "    \n",
    "    for speech_id, speech in speeches.items():\n",
    "        speech_size = len(speech)\n",
    "        \n",
    "        # If adding this speech would exceed max size and we already have speeches,\n",
    "        # start a new chunk\n",
    "        if current_size + speech_size > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = {}\n",
    "            current_size = 0\n",
    "            \n",
    "        current_chunk[speech_id] = speech\n",
    "        current_size += speech_size\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speech_topics(speeches: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's API to extract topics from the given speeches.\n",
    "    \"\"\"\n",
    "    # Prepare the speeches for analysis\n",
    "    speeches_text = \"\\n\\n\".join([f\"Speech ID: {id}\\nContent: {text}\" for id, text in speeches.items()])\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    Analyze each speech and identify its relevant topics. Topics should be chosen from this fixed list:\n",
    "                    - Governance and Democracy\n",
    "                    - Economy and Jobs\n",
    "                    - Health and Social Services\n",
    "                    - Education and Innovation\n",
    "                    - Environment and Energy\n",
    "                    - Defense and Security\n",
    "                    - Immigration and Border Policy\n",
    "                    - Justice and Civil Rights\n",
    "                    - Infrastructure and Transportation\n",
    "                    - Budget and Fiscal Responsibility\n",
    "\n",
    "                    For each speech, assign one or more topics that best match its content.\n",
    "                    Output should be in JSON format containing a list of objects, each with a speech_id and its corresponding topics list.\n",
    "                    Be precise and thorough in topic assignment.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": speeches_text\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": \n",
    "                    {\n",
    "                        \"name\":\"_\", \n",
    "                        \"schema\": Speeches.model_json_schema()\n",
    "                    }\n",
    "            },\n",
    "            temperature=0.3,\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech_file(file_path: str, output_dir: str, max_chunks: int=None):\n",
    "    \"\"\"\n",
    "    Process a speech file and save the results.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # Read speeches\n",
    "    speeches = read_speech_file(file_path)\n",
    "    print(f\"Found {len(speeches)} speeches\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = chunk_speeches(speeches)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Process each chunk and combine results\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        try:\n",
    "            # Get topics for speeches in this chunk\n",
    "            topics_json = extract_speech_topics(chunk)\n",
    "            topics_results = json.loads(topics_json)\n",
    "            \n",
    "            # Combine speech text with topics\n",
    "            for speech in topics_results[\"speeches\"]:\n",
    "                speech_id = speech[\"speech_id\"]\n",
    "                all_results[speech_id] = {\n",
    "                    \"speech\": speeches[speech_id],\n",
    "                    \"topics\": speech[\"topics\"]\n",
    "                }\n",
    "            \n",
    "            # print if chunk mod 10 == 0\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"Successfully processed chunk {i+1}\")\n",
    "\n",
    "            if max_chunks and i == max_chunks:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path).replace('.txt', '_gpt_topic_labels.json'))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech_files(speech_files: List[str], input_dir: str, output_dir: str, max_chunks: int=None):\n",
    "    \"\"\"\n",
    "    Process multiple speech files.\n",
    "    \"\"\"\n",
    "    for file_name in tqdm(speech_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        process_speech_file(file_path, output_dir, max_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_topic_frequencies(input_dir: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Analyze JSON files in the input directory and count topic frequencies.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing the speech analysis JSON files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping topics to their frequencies\n",
    "    \"\"\"\n",
    "    # Initialize counter for topics\n",
    "    topic_counter = Counter()\n",
    "    \n",
    "    # Get all JSON files in directory\n",
    "    json_files = [f for f in os.listdir(input_dir) if f.endswith('_gpt_topic_labels.json')]\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in tqdm(json_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Go through each speech\n",
    "            for speech_id, speech_data in data.items():\n",
    "                # Add each topic to our counter\n",
    "                topic_counter.update(speech_data['topics'])\n",
    "    \n",
    "    # Convert Counter to regular dictionary and sort by frequency\n",
    "    topic_frequencies = dict(sorted(topic_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return topic_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_statistics(topic_frequencies: Dict[str, int]):\n",
    "    \"\"\"\n",
    "    Print formatted statistics about topic frequencies.\n",
    "    \"\"\"\n",
    "    print(\"\\nTopic Frequencies:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find the longest topic name for formatting\n",
    "    max_topic_length = max(len(topic) for topic in topic_frequencies.keys())\n",
    "    \n",
    "    # Print each topic and its count\n",
    "    for topic, count in topic_frequencies.items():\n",
    "        print(f\"{topic:<{max_topic_length}} : {count:>6}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total topics mentioned: {sum(topic_frequencies.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:00<00:00, 95.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Frequencies:\n",
      "----------------------------------------\n",
      "Economy and Jobs                  :    512\n",
      "Governance and Democracy          :    464\n",
      "Health and Social Services        :    438\n",
      "Justice and Civil Rights          :    359\n",
      "Budget and Fiscal Responsibility  :    321\n",
      "Defense and Security              :    223\n",
      "Environment and Energy            :    180\n",
      "Infrastructure and Transportation :    146\n",
      "Education and Innovation          :    118\n",
      "Immigration and Border Policy     :     51\n",
      "Trade Policy                      :      4\n",
      "Culture and Community             :      2\n",
      "Community and Social Services     :      1\n",
      "Culture and Arts                  :      1\n",
      "Technology and Innovation         :      1\n",
      "Housing and Community Development :      1\n",
      "----------------------------------------\n",
      "Total topics mentioned: 2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"../small_speech_data\"\n",
    "    output_dir = \"outputs\"\n",
    "    speech_files = [f for f in os.listdir(input_dir) if f.startswith(\"speeches_\") and f.endswith(\".txt\")]\n",
    "    process_speech_files(speech_files, input_dir, output_dir, max_chunks=100) # changed from 10 to 100\n",
    "\n",
    "    # Analyze topic frequencies\n",
    "\n",
    "    input_dir_freq = \"outputs\"\n",
    "    topic_frequencies = analyze_topic_frequencies(input_dir_freq)\n",
    "    print_topic_statistics(topic_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
