{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model_path = \"/content/drive/MyDrive/issue classifier model \"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"/content/speeches_111_gpt_topic_labels.json\"\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to DataFrame\n",
    "records = [\n",
    "    {\"speech_id\": speech_id, \"speech_content\": details[\"speech\"], \"true_issues\": details[\"topics\"]}\n",
    "    for speech_id, details in data.items()\n",
    "]\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} speeches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(df):\n",
    "    # Remove speeches with only \"Governance and Democracy\" as their label\n",
    "    df = df[df[\"true_issues\"].apply(lambda x: x != [\"Governance and Democracy\"])].copy()\n",
    "\n",
    "    # Remove \"Governance and Democracy\" from labels for remaining speeches\n",
    "    df.loc[:, \"true_issues\"] = df[\"true_issues\"].apply(lambda x: [topic for topic in x if topic != \"Governance and Democracy\"])\n",
    "\n",
    "    # Remove speeches with empty content or no labels\n",
    "    df = df[df[\"speech_content\"].notnull()].copy()  # Remove empty speeches\n",
    "    df = df[df[\"speech_content\"].str.strip() != \"\"].copy()  # Remove speeches with only whitespace\n",
    "    df = df[df[\"true_issues\"].apply(len) > 0].copy()  # Remove speeches with no remaining labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df = preprocess_data(df)\n",
    "print(df.head())\n",
    "# Print summary\n",
    "print(f\"Preprocessed dataset contains {len(df)} speeches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unique topics from training\n",
    "unique_topics_path = \"/content/unique_topics.json\"\n",
    "with open(unique_topics_path, \"r\") as f:\n",
    "    unique_topics = json.load(f)\n",
    "\n",
    "print(\"Loaded unique topics:\", unique_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode topics for evaluation\n",
    "def encode_topics(topics):\n",
    "    labels = [0] * len(unique_topics)\n",
    "    for topic in topics:\n",
    "        labels[unique_topics.index(topic)] = 1\n",
    "    return labels\n",
    "\n",
    "df[\"true_issues_binary\"] = df[\"true_issues\"].apply(encode_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample speeches\n",
    "def sample_speeches(df, n):\n",
    "    return df.sample(n=n, random_state=42)\n",
    "\n",
    "# Input parameter: Number of speeches to sample\n",
    "n = 500  # Modify this to tweak the sample size\n",
    "sampled_df = sample_speeches(df, n)\n",
    "\n",
    "print(f\"Sampled {n} speeches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict issues for sampled speeches\n",
    "def predict_issues(model, tokenizer, speech, unique_topics, threshold=0.5):\n",
    "    encoding = tokenizer(\n",
    "        speech,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(model.device) for k, v in encoding.items()})\n",
    "        probabilities = torch.sigmoid(outputs.logits).cpu().numpy().flatten()\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    binary_predictions = (probabilities >= threshold).astype(int)\n",
    "    predicted_topics = [unique_topics[i] for i, label in enumerate(binary_predictions) if label == 1]\n",
    "    return predicted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict issues for the sampled speeches\n",
    "sampled_df[\"predicted_issues\"] = sampled_df[\"speech_content\"].apply(\n",
    "    lambda x: predict_issues(model, tokenizer, x, unique_topics)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode predicted issues as binary vectors\n",
    "sampled_df[\"predicted_issues_binary\"] = sampled_df[\"predicted_issues\"].apply(encode_topics)\n",
    "\n",
    "print(\"Predictions completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, classification_report, accuracy_score\n",
    "\n",
    "# Ensure true and predicted binary labels exist\n",
    "true_labels = np.array(sampled_df[\"true_issues_binary\"].tolist())\n",
    "predicted_labels = np.array(sampled_df[\"predicted_issues_binary\"].tolist())\n",
    "\n",
    "# Evaluate Hamming Loss\n",
    "hamming_loss_score = hamming_loss(true_labels, predicted_labels)\n",
    "print(f\"Hamming Loss: {hamming_loss_score:.4f}\")\n",
    "\n",
    "# Calculate Subset Accuracy\n",
    "subset_accuracy = np.mean(np.all(true_labels == predicted_labels, axis=1))\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "\n",
    "# Calculate Overall Accuracy\n",
    "overall_accuracy = accuracy_score(true_labels.flatten(), predicted_labels.flatten())\n",
    "print(f\"Overall Accuracy (flattened): {overall_accuracy:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=unique_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate confusion matrices for each label\n",
    "confusion_matrices = multilabel_confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Create a combined confusion matrix\n",
    "overall_confusion_matrix = np.sum(confusion_matrices, axis=0)\n",
    "\n",
    "# Plot the overall confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay(overall_confusion_matrix, display_labels=[\"No\", \"Yes\"]).plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Overall Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print the confusion matrix as raw values\n",
    "print(\"Overall Confusion Matrix (raw values):\")\n",
    "print(overall_confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
