{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodri/Desktop/MIT Fall 2024 Classes/6.8611/Project/NLP-Frameshifting/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertForSequenceClassification, RobertaModel\n",
    "from datetime import datetime\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model for axis classification\n",
    "\n",
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Unfreeze more layers since we have substantial data\n",
    "        for param in self.roberta.encoder.layer[-8:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Shared features layer\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use mean pooling instead of just [CLS] token\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Get shared features\n",
    "        shared_features = self.shared_features(pooled_output)\n",
    "        \n",
    "        # Get task-specific predictions\n",
    "        emotional_logits = self.emotional_classifier(shared_features)\n",
    "        political_logits = self.political_classifier(shared_features)\n",
    "        \n",
    "        return emotional_logits, political_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Core Functions\n",
    "\n",
    "def read_speech_file(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"Load speeches from a single congress file\"\"\"\n",
    "    speeches = {}\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            try:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) == 2:\n",
    "                    speech_id, speech = parts\n",
    "                    word_count = len(speech.split())\n",
    "                    if 35 < word_count < 400:\n",
    "                        speeches[speech_id] = speech\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return speeches\n",
    "\n",
    "def load_congress_data(congress_range: range, base_paths: Dict[str, str]) -> Dict[str, Dict[str, Dict]]:\n",
    "    \"\"\"Load speeches and party information from multiple congresses\"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for congress in tqdm(congress_range, desc=\"Loading congress data\"):\n",
    "        # Zero-pad congress number to 3 digits\n",
    "        congress_str = str(congress)\n",
    "        congress_str_padded = f\"{congress:03d}\"  # This will convert 79 to \"079\", 111 to \"111\", etc.\n",
    "        \n",
    "        # Determine which base path to use\n",
    "        if congress <= 111:\n",
    "            path = base_paths['bound']\n",
    "        else:\n",
    "            path = base_paths['daily']\n",
    "            \n",
    "        # Load speeches using padded number\n",
    "        speech_file = os.path.join(path, f\"speeches_{congress_str_padded}.txt\")\n",
    "        if not os.path.exists(speech_file):\n",
    "            print(f\"Could not find speech file: {speech_file}\")\n",
    "            continue\n",
    "            \n",
    "        # Read speeches\n",
    "        speeches = {}\n",
    "        with open(speech_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            next(file)  # Skip header\n",
    "            for line in file:\n",
    "                try:\n",
    "                    parts = line.strip().split('|')\n",
    "                    if len(parts) == 2:\n",
    "                        speech_id, speech = parts\n",
    "                        word_count = len(speech.split())\n",
    "                        if 35 < word_count < 400:\n",
    "                            speeches[speech_id] = {\"speech\": speech}\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # Load party information with padded number\n",
    "        speaker_map_file = os.path.join(path, f\"{congress_str_padded}_SpeakerMap.txt\")\n",
    "        if os.path.exists(speaker_map_file):\n",
    "            with open(speaker_map_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                header = file.readline().strip().split('|')\n",
    "                speech_id_idx = header.index('speech_id')\n",
    "                party_idx = header.index('party')\n",
    "                \n",
    "                for line in file:\n",
    "                    try:\n",
    "                        parts = line.strip().split('|')\n",
    "                        speech_id = parts[speech_id_idx]\n",
    "                        party = parts[party_idx]\n",
    "                        if speech_id in speeches:\n",
    "                            speeches[speech_id]['party'] = party\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        else:\n",
    "            print(f\"Could not find speaker map file: {speaker_map_file}\")\n",
    "        \n",
    "        # Only keep speeches with party information\n",
    "        speeches = {\n",
    "            k: v for k, v in speeches.items()\n",
    "            if 'party' in v and v['party'] in ['D', 'R']\n",
    "        }\n",
    "        \n",
    "        if speeches:\n",
    "            all_data[congress_str] = speeches\n",
    "            print(f\"Loaded {len(speeches)} speeches for congress {congress_str_padded}\")\n",
    "        else:\n",
    "            print(f\"No valid speeches found for congress {congress_str_padded}\")\n",
    "            \n",
    "    return all_data\n",
    "\n",
    "def load_party_data(congress_range: range, base_paths: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Load party affiliations for speakers\"\"\"\n",
    "    party_data = {}\n",
    "    \n",
    "    for congress in congress_range:\n",
    "        congress_str = str(congress)\n",
    "        \n",
    "        # Determine which base path to use\n",
    "        if congress <= 111:\n",
    "            path = base_paths['bound']\n",
    "        else:\n",
    "            path = base_paths['daily']\n",
    "            \n",
    "        desc_file = os.path.join(path, f\"descr_{congress_str}.txt\")\n",
    "        \n",
    "        if os.path.exists(desc_file):\n",
    "            with open(desc_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                next(file)  # Skip header\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        parts = line.strip().split('|')\n",
    "                        if len(parts) >= 2:\n",
    "                            speech_id = parts[0]\n",
    "                            party = parts[-1]  # Party is usually the last column\n",
    "                            if party in ['D', 'R']:  # Only keep Democrat and Republican\n",
    "                                party_data[speech_id] = party\n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "    return party_data\n",
    "\n",
    "class CongressionalAnalysis:\n",
    "    def __init__(self, \n",
    "                issue_model_path: str,\n",
    "                axis_model_path: str,\n",
    "                congress_range: range = range(79, 115)):\n",
    "        \"\"\"Initialize the analysis pipeline\"\"\"\n",
    "        self.congress_range = congress_range\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.VALID_ISSUES = {\n",
    "            'Economy and Jobs',\n",
    "            'Health and Social Services',\n",
    "            'Education and Innovation',\n",
    "            'Environment and Energy',\n",
    "            'Defense and Security',\n",
    "            'Immigration and Border Policy',\n",
    "            'Justice and Civil Rights',\n",
    "            'Infrastructure and Transportation',\n",
    "            'Budget and Fiscal Responsibility'\n",
    "        }\n",
    "\n",
    "        self.ISSUE_MAP = {\n",
    "            'LABEL_21': 'Economy and Jobs',\n",
    "            'LABEL_31': 'Health and Social Services',\n",
    "            'LABEL_22': 'Education and Innovation',\n",
    "            'LABEL_26': 'Environment and Energy',\n",
    "            'LABEL_19': 'Defense and Security',\n",
    "            'LABEL_43': 'Immigration and Border Policy',\n",
    "            'LABEL_47': 'Justice and Civil Rights',\n",
    "            'LABEL_44': 'Infrastructure and Transportation',\n",
    "            'LABEL_8': 'Budget and Fiscal Responsibility'\n",
    "        }\n",
    "        \n",
    "        # Load models\n",
    "        print(\"Loading models...\")\n",
    "        self.issue_model = self.load_issue_model(issue_model_path)\n",
    "        self.axis_model = self.load_axis_model(axis_model_path)\n",
    "        \n",
    "        # Create unique_issues list\n",
    "        self.unique_issues = list(self.issue_model.config.id2label.values())\n",
    "        print(f\"Loaded {len(self.unique_issues)} unique issues\")\n",
    "        \n",
    "        # Load tokenizers\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(issue_model_path)\n",
    "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Initialize data structures\n",
    "        self.speeches = {}\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_issue_model(self, model_path: str):\n",
    "        \"\"\"Load the issue classification model\"\"\"\n",
    "        return BertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    def load_axis_model(self, model_path: str):\n",
    "        \"\"\"Load the axis prediction model\"\"\"\n",
    "        model_state = torch.load(model_path, map_location=self.device)\n",
    "        model = PoliticalSpeechClassifier()\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        return model\n",
    "    \n",
    "    def load_data(self, base_paths: Dict[str, str], sample_size: int = 1000):\n",
    "        \"\"\"Load and sample speeches from each congress\"\"\"\n",
    "        print(\"Loading congress data...\")\n",
    "        all_data = load_congress_data(self.congress_range, base_paths)\n",
    "        \n",
    "        # Sample speeches from each congress\n",
    "        print(\"Sampling speeches...\")\n",
    "        for congress, speeches in all_data.items():\n",
    "            # Split by party\n",
    "            dem_speeches = {k: v for k, v in speeches.items() if v['party'] == 'D'}\n",
    "            rep_speeches = {k: v for k, v in speeches.items() if v['party'] == 'R'}\n",
    "            \n",
    "            per_party = sample_size // 2\n",
    "            sampled_speeches = {}\n",
    "            \n",
    "            # Only proceed if we have enough speeches from both parties\n",
    "            if len(dem_speeches) >= per_party and len(rep_speeches) >= per_party:\n",
    "                # Convert to list of items for sampling\n",
    "                dem_items = list(dem_speeches.items())\n",
    "                rep_items = list(rep_speeches.items())\n",
    "                \n",
    "                # Sample equally from each party\n",
    "                dem_sample = dict(random.sample(dem_items, per_party))\n",
    "                rep_sample = dict(random.sample(rep_items, per_party))\n",
    "                \n",
    "                sampled_speeches.update(dem_sample)\n",
    "                sampled_speeches.update(rep_sample)\n",
    "                \n",
    "                self.speeches[congress] = sampled_speeches\n",
    "            else:\n",
    "                print(f\"Warning: Not enough speeches from both parties in congress {congress}\")\n",
    "                print(f\"Democratic speeches: {len(dem_speeches)}\")\n",
    "                print(f\"Republican speeches: {len(rep_speeches)}\")\n",
    "\n",
    "# Part 2: Analysis Functions\n",
    "\n",
    "    def analyze_speeches(self):\n",
    "        \"\"\"Analyze all loaded speeches using both models\"\"\"\n",
    "        print(\"Analyzing speeches...\")\n",
    "        self.issue_model.to(self.device)\n",
    "        self.axis_model.to(self.device)\n",
    "        self.issue_model.eval()\n",
    "        self.axis_model.eval()\n",
    "\n",
    "        # Create unique_issues list before we need it\n",
    "        self.unique_issues = list(self.issue_model.config.id2label.values())\n",
    "\n",
    "        for congress, speeches in tqdm(self.speeches.items(), desc=\"Processing congresses\"):\n",
    "            congress_results = []\n",
    "\n",
    "            # print length of speeches\n",
    "            print(f\"Number of speeches in congress {congress}: {len(speeches)}\")\n",
    "            \n",
    "            # turn this for loop into one that uses tqdm\n",
    "            for speech_id, speech_data in tqdm(speeches.items(), desc=\"Processing speeches\"):\n",
    "            # for speech_id, speech_data in speeches.items():\n",
    "                try:\n",
    "                    # Extract data correctly from the speech_data dictionary\n",
    "                    speech_text = speech_data['speech']\n",
    "                    party = speech_data['party']\n",
    "                    \n",
    "                    if not speech_text or not party:\n",
    "                        continue\n",
    "\n",
    "                    # Predict issues\n",
    "                    issues = self.predict_issues(speech_text)\n",
    "                    \n",
    "                    # Predict axis scores\n",
    "                    axis_scores = self.predict_axis_scores(speech_text)\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'congress': int(congress),\n",
    "                        'speech_id': speech_id,\n",
    "                        'party': party,  # This should now be preserved\n",
    "                        'issues': issues,\n",
    "                        'emotional_intensity': axis_scores['emotional_intensity'],\n",
    "                        'political_spectrum': axis_scores['political_spectrum'],\n",
    "                        'emotional_confidence': axis_scores['emotional_confidence'],\n",
    "                        'political_confidence': axis_scores['political_confidence']\n",
    "                    }\n",
    "                    \n",
    "                    congress_results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing speech {speech_id}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if congress_results:\n",
    "                # Create DataFrame with explicit column ordering\n",
    "                self.analysis_results[congress] = pd.DataFrame(congress_results).fillna('')\n",
    "                \n",
    "                # Debug print to verify data\n",
    "                print(f\"\\nCongress {congress} results:\")\n",
    "                print(f\"Number of speeches processed: {len(congress_results)}\")\n",
    "                print(\"Columns:\", self.analysis_results[congress].columns.tolist())\n",
    "                print(\"Party distribution:\", self.analysis_results[congress]['party'].value_counts())\n",
    "    \n",
    "    def predict_issues(self, speech_text: str, threshold: float = 0.5) -> List[str]:\n",
    "        \"\"\"Predict issues with mapping to standard names\"\"\"\n",
    "        encoding = self.bert_tokenizer(\n",
    "            speech_text,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "            outputs = self.issue_model(**inputs)\n",
    "            probabilities = torch.sigmoid(outputs.logits).cpu().numpy().flatten()\n",
    "            \n",
    "            # Get predictions and map to standard names\n",
    "            raw_predictions = [\n",
    "                self.issue_model.config.id2label[i]\n",
    "                for i, prob in enumerate(probabilities)\n",
    "                if prob >= threshold\n",
    "            ]\n",
    "            \n",
    "            # Filter to only valid issues\n",
    "            valid_predictions = [\n",
    "                issue for issue in raw_predictions\n",
    "                if issue in self.ISSUE_MAP\n",
    "            ]\n",
    "            \n",
    "            return valid_predictions\n",
    "    \n",
    "    def predict_axis_scores(self, speech_text: str) -> Dict:\n",
    "        \"\"\"Predict axis scores for a single speech\"\"\"\n",
    "        encoding = self.roberta_tokenizer(\n",
    "            speech_text,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "            emotional_logits, political_logits = self.axis_model(**inputs)\n",
    "            \n",
    "            emotional_probs = torch.softmax(emotional_logits, dim=1)\n",
    "            political_probs = torch.softmax(political_logits, dim=1)\n",
    "            \n",
    "            emotional_pred = torch.argmax(emotional_probs, dim=1).item() + 1\n",
    "            political_pred = torch.argmax(political_probs, dim=1).item() + 1\n",
    "            \n",
    "            emotional_conf = emotional_probs[0][emotional_pred-1].item()\n",
    "            political_conf = political_probs[0][political_pred-1].item()\n",
    "        \n",
    "        return {\n",
    "            'emotional_intensity': emotional_pred,\n",
    "            'emotional_confidence': emotional_conf,\n",
    "            'political_spectrum': political_pred,\n",
    "            'political_confidence': political_conf\n",
    "        }\n",
    "    \n",
    "    def analyze_framing_shifts(self, save_dir='analysis_results'):\n",
    "        \"\"\"Analyze framing shifts over time\"\"\"\n",
    "        print(\"Analyzing framing shifts...\")\n",
    "        \n",
    "        # Initialize storage for trends\n",
    "        self.trends = {\n",
    "            'by_party': defaultdict(list),\n",
    "            'by_issue': defaultdict(list),\n",
    "            'by_party_issue': defaultdict(lambda: defaultdict(list))\n",
    "        }\n",
    "\n",
    "        # Initialize metrics tracking\n",
    "        self.metrics = {\n",
    "            'global': {\n",
    "                'emotional_volatility': [],\n",
    "                'political_volatility': [],\n",
    "                'party_divergence': []\n",
    "            },\n",
    "            'by_party': defaultdict(lambda: {\n",
    "                'emotional_trend': [],\n",
    "                'political_trend': []\n",
    "            })\n",
    "        }\n",
    "\n",
    "        # Process each congress\n",
    "        for congress, df in self.analysis_results.items():\n",
    "            if 'party' not in df.columns:\n",
    "                print(f\"Warning: No party information for congress {congress}\")\n",
    "                continue\n",
    "            congress_num = int(congress)\n",
    "            \n",
    "            # Analyze by party\n",
    "            for party in ['D', 'R']:\n",
    "                party_df = df[df['party'] == party]\n",
    "                if not party_df.empty:\n",
    "                    self.trends['by_party'][party].append({\n",
    "                        'congress': congress_num,\n",
    "                        'emotional_avg': party_df['emotional_intensity'].mean(),\n",
    "                        'emotional_std': party_df['emotional_intensity'].std(),\n",
    "                        'political_avg': party_df['political_spectrum'].mean(),\n",
    "                        'political_std': party_df['political_spectrum'].std(),\n",
    "                        'count': len(party_df)\n",
    "                    })\n",
    "                    \n",
    "                    # Track party trends (moved inside party loop)\n",
    "                    self.metrics['by_party'][party]['emotional_trend'].append({\n",
    "                        'congress': congress_num,\n",
    "                        'mean': party_df['emotional_intensity'].mean(),\n",
    "                        'std': party_df['emotional_intensity'].std()\n",
    "                    })\n",
    "            \n",
    "            # Analyze by issue\n",
    "            for issue in self.unique_issues:\n",
    "                if issue in self.ISSUE_MAP:\n",
    "                    issue_mask = df['issues'].apply(lambda x: issue in x)\n",
    "                    issue_df = df[issue_mask]\n",
    "                    if not issue_df.empty:\n",
    "                        self.trends['by_issue'][issue].append({\n",
    "                            'congress': congress_num,\n",
    "                            'emotional_avg': issue_df['emotional_intensity'].mean(),\n",
    "                            'emotional_std': issue_df['emotional_intensity'].std(),\n",
    "                            'political_avg': issue_df['political_spectrum'].mean(),\n",
    "                            'political_std': issue_df['political_spectrum'].std(),\n",
    "                            'count': len(issue_df)\n",
    "                        })\n",
    "                        \n",
    "                        # Analyze by party within issue\n",
    "                        for party in ['D', 'R']:\n",
    "                            party_issue_df = issue_df[issue_df['party'] == party]\n",
    "                            if not party_issue_df.empty:\n",
    "                                self.trends['by_party_issue'][issue][party].append({\n",
    "                                    'congress': congress_num,\n",
    "                                    'emotional_avg': party_issue_df['emotional_intensity'].mean(),\n",
    "                                    'emotional_std': party_issue_df['emotional_intensity'].std(),\n",
    "                                    'political_avg': party_issue_df['political_spectrum'].mean(),\n",
    "                                    'political_std': party_issue_df['political_spectrum'].std(),\n",
    "                                    'count': len(party_issue_df)\n",
    "                                })\n",
    "            \n",
    "            # Add volatility metrics\n",
    "            self.metrics['global']['emotional_volatility'].append({\n",
    "                'congress': congress_num,\n",
    "                'std': df['emotional_intensity'].std()\n",
    "            })\n",
    "            self.metrics['global']['political_volatility'].append({\n",
    "                'congress': congress_num,\n",
    "                'std': df['political_spectrum'].std()\n",
    "            })\n",
    "\n",
    "        # Convert trend data to DataFrames\n",
    "        self.trend_dfs = {\n",
    "            'by_party': {\n",
    "                party: pd.DataFrame(data)\n",
    "                for party, data in self.trends['by_party'].items()\n",
    "            },\n",
    "            'by_issue': {\n",
    "                issue: pd.DataFrame(data)\n",
    "                for issue, data in self.trends['by_issue'].items()\n",
    "            },\n",
    "            'by_party_issue': {\n",
    "                issue: {\n",
    "                    party: pd.DataFrame(data)\n",
    "                    for party, data in party_data.items()\n",
    "                }\n",
    "                for issue, party_data in self.trends['by_party_issue'].items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save enhanced metrics\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(os.path.join(save_dir, 'detailed_metrics.json'), 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "\n",
    "# Part 3: Visualization and Metrics\n",
    "\n",
    "    def calculate_polarization_metrics(self):\n",
    "        \"\"\"Calculate polarization metrics over time\"\"\"\n",
    "        print(\"Calculating polarization metrics...\")\n",
    "        \n",
    "        self.polarization_metrics = defaultdict(list)\n",
    "        \n",
    "        # Overall polarization\n",
    "        for congress in sorted(self.analysis_results.keys()):\n",
    "            df = self.analysis_results[congress]\n",
    "            dem_df = df[df['party'] == 'D']\n",
    "            rep_df = df[df['party'] == 'R']\n",
    "            \n",
    "            metrics = {\n",
    "                'congress': int(congress),\n",
    "                'emotional_gap': (rep_df['emotional_intensity'].mean() - \n",
    "                                dem_df['emotional_intensity'].mean()),\n",
    "                'political_gap': (rep_df['political_spectrum'].mean() - \n",
    "                                dem_df['political_spectrum'].mean()),\n",
    "                'emotional_overlap': self._calculate_distribution_overlap(\n",
    "                    dem_df['emotional_intensity'], rep_df['emotional_intensity']\n",
    "                ),\n",
    "                'political_overlap': self._calculate_distribution_overlap(\n",
    "                    dem_df['political_spectrum'], rep_df['political_spectrum']\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            self.polarization_metrics['overall'].append(metrics)\n",
    "        \n",
    "        # By issue polarization\n",
    "        for issue in self.unique_issues:\n",
    "            if issue in self.ISSUE_MAP:\n",
    "                for congress in sorted(self.analysis_results.keys()):\n",
    "                    df = self.analysis_results[congress]\n",
    "                    issue_mask = df['issues'].apply(lambda x: issue in x)\n",
    "                    issue_df = df[issue_mask]\n",
    "                    \n",
    "                    if len(issue_df) > 10:  # Only calculate if enough samples\n",
    "                        dem_df = issue_df[issue_df['party'] == 'D']\n",
    "                        rep_df = issue_df[issue_df['party'] == 'R']\n",
    "                        \n",
    "                        if len(dem_df) > 5 and len(rep_df) > 5:\n",
    "                            metrics = {\n",
    "                                'congress': int(congress),\n",
    "                                'emotional_gap': (rep_df['emotional_intensity'].mean() - \n",
    "                                                dem_df['emotional_intensity'].mean()),\n",
    "                                'political_gap': (rep_df['political_spectrum'].mean() - \n",
    "                                                dem_df['political_spectrum'].mean()),\n",
    "                                'emotional_overlap': self._calculate_distribution_overlap(\n",
    "                                    dem_df['emotional_intensity'], rep_df['emotional_intensity']\n",
    "                                ),\n",
    "                                'political_overlap': self._calculate_distribution_overlap(\n",
    "                                    dem_df['political_spectrum'], rep_df['political_spectrum']\n",
    "                                )\n",
    "                            }\n",
    "                            \n",
    "                            self.polarization_metrics[issue].append(metrics)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        self.polarization_dfs = {\n",
    "            key: pd.DataFrame(data)\n",
    "            for key, data in self.polarization_metrics.items()\n",
    "        }\n",
    "    \n",
    "    def _calculate_distribution_overlap(self, dist1, dist2):\n",
    "        \"\"\"Calculate overlap between two distributions\"\"\"\n",
    "        hist1, bins = np.histogram(dist1, bins=5, density=True)\n",
    "        hist2, _ = np.histogram(dist2, bins=bins, density=True)\n",
    "        return np.minimum(hist1, hist2).sum() * (bins[1] - bins[0])\n",
    "\n",
    "    def _plot_issue_heatmaps(self, save_dir):\n",
    "        \"\"\"Create heatmaps showing issue prevalence and characteristics over time\"\"\"\n",
    "        # Create directory for issue heatmaps\n",
    "        heatmap_dir = os.path.join(save_dir, 'issue_heatmaps')\n",
    "        os.makedirs(heatmap_dir, exist_ok=True)\n",
    "        \n",
    "        # Prepare data for heatmaps\n",
    "        congresses = sorted(self.analysis_results.keys())\n",
    "        issues = list(self.ISSUE_MAP.values())\n",
    "        \n",
    "        # Initialize matrices for different metrics\n",
    "        prevalence_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        emotional_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        political_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        \n",
    "        # Fill matrices\n",
    "        for i, issue in enumerate(issues):\n",
    "            for j, congress in enumerate(congresses):\n",
    "                df = self.analysis_results[congress]\n",
    "                # Find the LABEL that maps to this issue\n",
    "                issue_label = [k for k, v in self.ISSUE_MAP.items() if v == issue][0]\n",
    "                issue_mask = df['issues'].apply(lambda x: issue_label in x)\n",
    "                issue_df = df[issue_mask]\n",
    "                \n",
    "                if not issue_df.empty:\n",
    "                    prevalence_matrix[i, j] = len(issue_df) / len(df) * 100\n",
    "                    emotional_matrix[i, j] = issue_df['emotional_intensity'].mean()\n",
    "                    political_matrix[i, j] = issue_df['political_spectrum'].mean()\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(prevalence_matrix, \n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='YlOrRd',\n",
    "                    annot=True,\n",
    "                    fmt='.1f')\n",
    "        plt.title('Issue Prevalence Over Time (%)')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_prevalence.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(emotional_matrix,\n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='RdBu_r',\n",
    "                    annot=True,\n",
    "                    fmt='.2f',\n",
    "                    vmin=1, vmax=5)\n",
    "        plt.title('Average Emotional Intensity by Issue Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_emotional.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(political_matrix,\n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='RdBu_r',\n",
    "                    annot=True,\n",
    "                    fmt='.2f',\n",
    "                    vmin=1, vmax=5)\n",
    "        plt.title('Average Political Position by Issue Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_political.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def calculate_issue_dynamics(self):\n",
    "        \"\"\"Calculate how issues change over time\"\"\"\n",
    "        dynamics = {}\n",
    "        \n",
    "        for issue in self.ISSUE_MAP.values():\n",
    "            dynamics[issue] = {\n",
    "                'volatility': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                },\n",
    "                'trend': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                },\n",
    "                'party_gap': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Calculate metrics per congress\n",
    "            for congress in sorted(self.analysis_results.keys()):\n",
    "                df = self.analysis_results[congress]\n",
    "                issue_label = [k for k, v in self.ISSUE_MAP.items() if v == issue][0]\n",
    "                issue_mask = df['issues'].apply(lambda x: issue_label in x)\n",
    "                issue_df = df[issue_mask]\n",
    "                \n",
    "                if len(issue_df) > 10:  # Only calculate if enough samples\n",
    "                    # Volatility (standard deviation)\n",
    "                    dynamics[issue]['volatility']['emotional'].append(\n",
    "                        issue_df['emotional_intensity'].std()\n",
    "                    )\n",
    "                    dynamics[issue]['volatility']['political'].append(\n",
    "                        issue_df['political_spectrum'].std()\n",
    "                    )\n",
    "                    \n",
    "                    # Party differences\n",
    "                    dem_df = issue_df[issue_df['party'] == 'D']\n",
    "                    rep_df = issue_df[issue_df['party'] == 'R']\n",
    "                    \n",
    "                    if len(dem_df) > 5 and len(rep_df) > 5:\n",
    "                        dynamics[issue]['party_gap']['emotional'].append(\n",
    "                            rep_df['emotional_intensity'].mean() - dem_df['emotional_intensity'].mean()\n",
    "                        )\n",
    "                        dynamics[issue]['party_gap']['political'].append(\n",
    "                            rep_df['political_spectrum'].mean() - dem_df['political_spectrum'].mean()\n",
    "                        )\n",
    "        \n",
    "        return dynamics\n",
    "    \n",
    "    def plot_framing_trends(self, save_dir='plots'):\n",
    "        \"\"\"Generate plots for framing trends\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Plot overall party trends\n",
    "        self._plot_party_trends(save_dir)\n",
    "        \n",
    "        # Plot issue-specific trends\n",
    "        self._plot_issue_trends(save_dir)\n",
    "        \n",
    "        # Plot polarization trends\n",
    "        self._plot_polarization_trends(save_dir)\n",
    "\n",
    "        # New issue analysis plots\n",
    "        self._plot_issue_heatmaps(save_dir)\n",
    "\n",
    "        # Calculate and save issue dynamics\n",
    "        dynamics = self.calculate_issue_dynamics()\n",
    "        with open(os.path.join(save_dir, 'issue_dynamics.json'), 'w') as f:\n",
    "            json.dump(dynamics, f, indent=2)\n",
    "    \n",
    "    def _plot_party_trends(self, save_dir):\n",
    "        \"\"\"Plot party-level trends with standardized scales\"\"\"\n",
    "        # Emotional Intensity by Party\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for party in ['D', 'R']:\n",
    "            df = self.trend_dfs['by_party'][party]\n",
    "            plt.plot(df['congress'], df['emotional_avg'], \n",
    "                    label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "            plt.fill_between(df['congress'],\n",
    "                            df['emotional_avg'] - df['emotional_std'],\n",
    "                            df['emotional_avg'] + df['emotional_std'],\n",
    "                            alpha=0.2)\n",
    "        \n",
    "        plt.title('Emotional Intensity by Party Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Average Emotional Intensity')\n",
    "        plt.ylim(1, 5)  # Set fixed scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/emotional_intensity_by_party.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Political Spectrum by Party\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for party in ['D', 'R']:\n",
    "            df = self.trend_dfs['by_party'][party]\n",
    "            plt.plot(df['congress'], df['political_avg'],\n",
    "                    label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "            plt.fill_between(df['congress'],\n",
    "                            df['political_avg'] - df['political_std'],\n",
    "                            df['political_avg'] + df['political_std'],\n",
    "                            alpha=0.2)\n",
    "        \n",
    "        plt.title('Political Spectrum Position by Party Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Average Political Spectrum Position')\n",
    "        plt.ylim(1, 5)  # Set fixed scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/political_spectrum_by_party.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_issue_trends(self, save_dir):\n",
    "        \"\"\"Plot issue-level trends\"\"\"\n",
    "        # Skip empty or invalid issues\n",
    "        if not self.trends['by_issue']:\n",
    "            return\n",
    "\n",
    "        for issue in self.ISSUE_MAP.values():  # Use our defined issue map\n",
    "            # find the key in self.ISSUE_MAP that corresponds to the issue\n",
    "            for key, value in self.ISSUE_MAP.items():\n",
    "                if value == issue:\n",
    "                    issue_key = key\n",
    "                    break\n",
    "            if issue_key not in self.trend_dfs['by_issue']:\n",
    "                print(f\"Skipping issue: {issue_key}, whose real name is {issue}\")\n",
    "                print(self.trend_dfs['by_issue'].keys())\n",
    "                continue\n",
    "                \n",
    "            # Create directory for issue-specific plots\n",
    "            issue_dir = os.path.join(save_dir, 'issues', issue.lower().replace(' ', '_'))\n",
    "            os.makedirs(issue_dir, exist_ok=True)\n",
    "            \n",
    "            # Emotional Intensity\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for party in ['D', 'R']:\n",
    "                if issue_key in self.trend_dfs['by_party_issue'] and \\\n",
    "                party in self.trend_dfs['by_party_issue'][issue_key]:\n",
    "                    df = self.trend_dfs['by_party_issue'][issue_key][party]\n",
    "                    plt.plot(df['congress'], df['emotional_avg'],\n",
    "                            label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "                    plt.fill_between(df['congress'],\n",
    "                                df['emotional_avg'] - df['emotional_std'],\n",
    "                                df['emotional_avg'] + df['emotional_std'],\n",
    "                                alpha=0.2)\n",
    "            \n",
    "            plt.title(f'Emotional Intensity Over Time: {issue}')\n",
    "            plt.xlabel('Congress')\n",
    "            plt.ylabel('Average Emotional Intensity')\n",
    "            plt.ylim(1, 5)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{issue_dir}/emotional_intensity.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Political Spectrum\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for party in ['D', 'R']:\n",
    "                if issue_key in self.trend_dfs['by_party_issue'] and \\\n",
    "                party in self.trend_dfs['by_party_issue'][issue_key]:\n",
    "                    df = self.trend_dfs['by_party_issue'][issue_key][party]\n",
    "                    plt.plot(df['congress'], df['political_avg'],\n",
    "                            label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "                    plt.fill_between(df['congress'],\n",
    "                                df['political_avg'] - df['political_std'],\n",
    "                                df['political_avg'] + df['political_std'],\n",
    "                                alpha=0.2)\n",
    "            \n",
    "            plt.title(f'Political Spectrum Position Over Time: {issue}')\n",
    "            plt.xlabel('Congress')\n",
    "            plt.ylabel('Average Political Spectrum Position')\n",
    "            plt.ylim(1, 5)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{issue_dir}/political_spectrum.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    def _plot_polarization_trends(self, save_dir):\n",
    "        \"\"\"Plot polarization trends with standardized scales\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df = self.polarization_dfs['overall']\n",
    "        plt.plot(df['congress'], df['emotional_gap'], label='Emotional Gap')\n",
    "        plt.plot(df['congress'], df['political_gap'], label='Political Gap')\n",
    "        plt.title('Party Polarization Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Party Gap')\n",
    "        plt.ylim(-4, 4)  # Maximum possible gap is ±4 on a 1-5 scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/overall_polarization.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing analysis pipeline...\n",
      "Using device: cpu\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/xhv26dgs4tscxfcc4mdbh8zw0000gn/T/ipykernel_61178/1724331458.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(model_path, map_location=self.device)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74 unique issues\n",
      "\n",
      "Loading congressional data...\n",
      "Loading congress data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   3%|▎         | 1/36 [00:01<00:52,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68687 speeches for congress 079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   6%|▌         | 2/36 [00:02<00:46,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55732 speeches for congress 080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   8%|▊         | 3/36 [00:04<00:46,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 91333 speeches for congress 081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  11%|█         | 4/36 [00:05<00:41,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71327 speeches for congress 082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  14%|█▍        | 5/36 [00:06<00:42,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71976 speeches for congress 083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  17%|█▋        | 6/36 [00:08<00:43,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56304 speeches for congress 084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  19%|█▉        | 7/36 [00:10<00:47,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 76595 speeches for congress 085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  22%|██▏       | 8/36 [00:13<00:54,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 77945 speeches for congress 086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  25%|██▌       | 9/36 [00:15<00:57,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78954 speeches for congress 087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  28%|██▊       | 10/36 [00:18<01:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82647 speeches for congress 088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  31%|███       | 11/36 [00:20<00:56,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82540 speeches for congress 089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  33%|███▎      | 12/36 [00:22<00:53,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89339 speeches for congress 090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  36%|███▌      | 13/36 [00:24<00:52,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 93230 speeches for congress 091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  39%|███▉      | 14/36 [00:27<00:49,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 79993 speeches for congress 092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  42%|████▏     | 15/36 [00:29<00:47,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90806 speeches for congress 093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  44%|████▍     | 16/36 [00:31<00:46,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 94878 speeches for congress 094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  47%|████▋     | 17/36 [00:34<00:44,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99770 speeches for congress 095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  50%|█████     | 18/36 [00:36<00:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 88242 speeches for congress 096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  53%|█████▎    | 19/36 [00:38<00:37,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71751 speeches for congress 097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  56%|█████▌    | 20/36 [00:40<00:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 73694 speeches for congress 098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  58%|█████▊    | 21/36 [00:42<00:32,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74726 speeches for congress 099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  61%|██████    | 22/36 [00:44<00:30,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68877 speeches for congress 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  64%|██████▍   | 23/36 [00:46<00:27,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61333 speeches for congress 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  67%|██████▋   | 24/36 [00:48<00:24,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59940 speeches for congress 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  69%|██████▉   | 25/36 [00:50<00:22,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60331 speeches for congress 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  72%|███████▏  | 26/36 [00:53<00:20,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 72521 speeches for congress 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  75%|███████▌  | 27/36 [00:54<00:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50320 speeches for congress 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  78%|███████▊  | 28/36 [00:56<00:15,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50657 speeches for congress 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  81%|████████  | 29/36 [00:58<00:12,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41234 speeches for congress 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  83%|████████▎ | 30/36 [00:59<00:10,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45099 speeches for congress 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  86%|████████▌ | 31/36 [01:01<00:08,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44499 speeches for congress 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  89%|████████▉ | 32/36 [01:03<00:07,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49258 speeches for congress 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  92%|█████████▏| 33/36 [01:04<00:05,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44920 speeches for congress 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  94%|█████████▍| 34/36 [01:06<00:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34665 speeches for congress 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  97%|█████████▋| 35/36 [01:07<00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30332 speeches for congress 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data: 100%|██████████| 36/36 [01:08<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27075 speeches for congress 114\n",
      "Sampling speeches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing speeches...\n",
      "Analyzing speeches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing congresses:   0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches in congress 79: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:11<00:00,  2.03it/s]\n",
      "Processing congresses:   3%|▎         | 1/36 [08:11<4:46:43, 491.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 79 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 80: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:18<00:00,  2.01it/s]\n",
      "Processing congresses:   6%|▌         | 2/36 [16:30<4:40:53, 495.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 80 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 81: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:27<00:00,  1.97it/s]\n",
      "Processing congresses:   8%|▊         | 3/36 [24:57<4:35:30, 500.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 81 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 82: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:31<00:00,  1.95it/s]\n",
      "Processing congresses:  11%|█         | 4/36 [33:28<4:29:24, 505.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 82 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 83: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:24<00:00,  1.98it/s]\n",
      "Processing congresses:  14%|█▍        | 5/36 [41:53<4:20:49, 504.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 83 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 84: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "Processing congresses:  17%|█▋        | 6/36 [51:12<4:21:40, 523.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 84 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 85: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:23<00:00,  1.78it/s]\n",
      "Processing congresses:  19%|█▉        | 7/36 [1:00:35<4:19:16, 536.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 85 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 86: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:49<00:00,  1.70it/s]\n",
      "Processing congresses:  22%|██▏       | 8/36 [1:10:25<4:18:14, 553.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 86 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 87: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:18<00:00,  1.79it/s]\n",
      "Processing congresses:  25%|██▌       | 9/36 [1:19:43<4:09:41, 554.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 87 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 88: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [24:15<00:00,  1.46s/it]\n",
      "Processing congresses:  28%|██▊       | 10/36 [1:43:58<6:00:52, 832.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 88 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 89: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [21:19<00:00,  1.28s/it]\n",
      "Processing congresses:  31%|███       | 11/36 [2:05:18<6:43:56, 969.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 89 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 90: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:36<00:00,  1.94it/s]\n",
      "Processing congresses:  33%|███▎      | 12/36 [2:13:54<5:32:38, 831.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 90 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 91: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:43<00:00,  1.91it/s]\n",
      "Processing congresses:  36%|███▌      | 13/36 [2:22:37<4:42:58, 738.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 91 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 92: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:54<00:00,  1.87it/s]\n",
      "Processing congresses:  39%|███▉      | 14/36 [2:31:32<4:08:10, 676.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 92 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 93: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:41<00:00,  1.72it/s]\n",
      "Processing congresses:  42%|████▏     | 15/36 [2:41:13<3:46:48, 648.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 93 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 94: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [10:49<00:00,  1.54it/s]\n",
      "Processing congresses:  44%|████▍     | 16/36 [2:52:03<3:36:09, 648.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 94 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 95: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [11:02<00:00,  1.51it/s]\n",
      "Processing congresses:  47%|████▋     | 17/36 [3:03:05<3:26:40, 652.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 95 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 96: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [10:06<00:00,  1.65it/s]\n",
      "Processing congresses:  50%|█████     | 18/36 [3:13:12<3:11:38, 638.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 96 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 97: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:27<00:00,  1.76it/s]\n",
      "Processing congresses:  53%|█████▎    | 19/36 [3:22:39<2:54:53, 617.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 97 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 98: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:54<00:00,  1.68it/s]\n",
      "Processing congresses:  56%|█████▌    | 20/36 [3:32:34<2:42:47, 610.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 98 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 99: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [12:28<00:00,  1.34it/s]\n",
      "Processing congresses:  58%|█████▊    | 21/36 [3:45:02<2:42:57, 651.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 99 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 100: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [11:57<00:00,  1.39it/s]\n",
      "Processing congresses:  61%|██████    | 22/36 [3:56:59<2:36:40, 671.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 100 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 101: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [12:18<00:00,  1.35it/s]\n",
      "Processing congresses:  64%|██████▍   | 23/36 [4:09:18<2:29:51, 691.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 101 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 102: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "Processing congresses:  67%|██████▋   | 24/36 [4:18:37<2:10:23, 651.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 102 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 103: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:09<00:00,  1.82it/s]\n",
      "Processing congresses:  69%|██████▉   | 25/36 [4:27:47<1:53:54, 621.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 103 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 104: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:50<00:00,  1.88it/s]\n",
      "Processing congresses:  72%|███████▏  | 26/36 [4:36:38<1:39:00, 594.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 104 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 105: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:53<00:00,  1.87it/s]\n",
      "Processing congresses:  75%|███████▌  | 27/36 [4:45:31<1:26:24, 576.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 105 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 106: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:20<00:00,  1.78it/s]\n",
      "Processing congresses:  78%|███████▊  | 28/36 [4:54:52<1:16:11, 571.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 106 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 107: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:50<00:00,  1.69it/s]\n",
      "Processing congresses:  81%|████████  | 29/36 [5:04:43<1:07:20, 577.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 107 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 108: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:48<00:00,  1.70it/s]\n",
      "Processing congresses:  83%|████████▎ | 30/36 [5:14:32<58:04, 580.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 108 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 109: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:49<00:00,  1.70it/s]\n",
      "Processing congresses:  86%|████████▌ | 31/36 [5:24:21<48:36, 583.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 109 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 110: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:50<00:00,  1.69it/s]\n",
      "Processing congresses:  89%|████████▉ | 32/36 [5:34:11<39:01, 585.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 110 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 111: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:50<00:00,  1.69it/s]\n",
      "Processing congresses:  92%|█████████▏| 33/36 [5:44:02<29:20, 586.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 111 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 112: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:51<00:00,  1.69it/s]\n",
      "Processing congresses:  94%|█████████▍| 34/36 [5:53:53<19:36, 588.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 112 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 113: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:47<00:00,  1.70it/s]\n",
      "Processing congresses:  97%|█████████▋| 35/36 [6:03:40<09:47, 587.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 113 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 114: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:57<00:00,  1.67it/s]\n",
      "Processing congresses: 100%|██████████| 36/36 [6:13:38<00:00, 622.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 114 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Analyzing framing shifts...\n",
      "Analyzing framing shifts...\n",
      "\n",
      "Calculating polarization metrics...\n",
      "Calculating polarization metrics...\n",
      "\n",
      "Generating visualization plots...\n",
      "\n",
      "Saving analysis results...\n",
      "\n",
      "Generating summary report...\n",
      "\n",
      "Analysis complete! Results saved to 'analysis_results' directory.\n",
      "\n",
      "Generated files:\n",
      "1. analysis_results/plots/ - Visualization plots\n",
      "2. analysis_results/trend_data.json - Raw trend data\n",
      "3. analysis_results/summary_report.txt - Analysis summary\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Main Execution\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    base_paths = {\n",
    "        'bound': \"../hein-bound\",  # Path to bound speeches (79-111)\n",
    "        'daily': \"../hein-daily\"   # Path to daily speeches (112-114)\n",
    "    }\n",
    "    \n",
    "    # Model paths\n",
    "    issue_model_path = \"../issue_classifier_eval/model/saved_issue_model\"  # Path to saved issue classification model\n",
    "    axis_model_path = \"../large-training-output/model_artifacts_20241202_142615/model.pt\"  # Path to saved axis prediction model\n",
    "    \n",
    "    # Initialize analysis\n",
    "    print(\"Initializing analysis pipeline...\")\n",
    "    analyzer = CongressionalAnalysis(\n",
    "        issue_model_path=issue_model_path,\n",
    "        axis_model_path=axis_model_path,\n",
    "        congress_range=range(79, 115)\n",
    "    )\n",
    "    \n",
    "    # Load and process data\n",
    "    print(\"\\nLoading congressional data...\")\n",
    "    analyzer.load_data(\n",
    "        base_paths=base_paths,\n",
    "        sample_size=1000\n",
    "    )\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"\\nAnalyzing speeches...\")\n",
    "    analyzer.analyze_speeches()\n",
    "    \n",
    "    # Analyze framing shifts\n",
    "    print(\"\\nAnalyzing framing shifts...\")\n",
    "    analyzer.analyze_framing_shifts()\n",
    "    \n",
    "    # Calculate polarization metrics\n",
    "    print(\"\\nCalculating polarization metrics...\")\n",
    "    analyzer.calculate_polarization_metrics()\n",
    "    \n",
    "    # Generate plots\n",
    "    print(\"\\nGenerating visualization plots...\")\n",
    "    analyzer.plot_framing_trends(save_dir='analysis_results/plots')\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nSaving analysis results...\")\n",
    "    results_dir = 'analysis_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save trend data\n",
    "    trend_data = {\n",
    "        'by_party': {\n",
    "            party: df.to_dict('records')\n",
    "            for party, df in analyzer.trend_dfs['by_party'].items()\n",
    "        },\n",
    "        'by_issue': {\n",
    "            analyzer.ISSUE_MAP[issue]: df.to_dict('records') # issue_name = analyzer.ISSUE_MAP[issue]\n",
    "            for issue, df in analyzer.trend_dfs['by_issue'].items()\n",
    "        },\n",
    "        'polarization': {\n",
    "            (analyzer.ISSUE_MAP[issue] if issue in analyzer.ISSUE_MAP else issue): df.to_dict('records')\n",
    "            for issue, df in analyzer.polarization_dfs.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/trend_data.json\", 'w') as f:\n",
    "        json.dump(trend_data, f, indent=2)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\nGenerating summary report...\")\n",
    "    with open(f\"{results_dir}/summary_report.txt\", 'w') as f:\n",
    "        f.write(\"Congressional Speech Analysis Summary\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        \n",
    "        f.write(\"Analysis Parameters:\\n\")\n",
    "        f.write(f\"- Congress Range: 79-114\\n\")\n",
    "        f.write(f\"- Speeches per Congress: 1000\\n\")\n",
    "        f.write(f\"- Total Speeches Analyzed: {sum(len(df) for df in analyzer.analysis_results.values())}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Overall Trends:\\n\")\n",
    "        f.write(\"--------------\\n\")\n",
    "        for party in ['D', 'R']:\n",
    "            party_name = 'Democratic' if party == 'D' else 'Republican'\n",
    "            df = analyzer.trend_dfs['by_party'][party]\n",
    "            \n",
    "            f.write(f\"\\n{party_name} Party:\\n\")\n",
    "            f.write(f\"- Emotional Intensity Change: {df['emotional_avg'].iloc[-1] - df['emotional_avg'].iloc[0]:.2f}\\n\")\n",
    "            f.write(f\"- Political Position Change: {df['political_avg'].iloc[-1] - df['political_avg'].iloc[0]:.2f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nPolarization Analysis:\\n\")\n",
    "        f.write(\"---------------------\\n\")\n",
    "        df = analyzer.polarization_dfs['overall']\n",
    "        f.write(f\"- Initial Emotional Gap: {df['emotional_gap'].iloc[0]:.2f}\\n\")\n",
    "        f.write(f\"- Final Emotional Gap: {df['emotional_gap'].iloc[-1]:.2f}\\n\")\n",
    "        f.write(f\"- Initial Political Gap: {df['political_gap'].iloc[0]:.2f}\\n\")\n",
    "        f.write(f\"- Final Political Gap: {df['political_gap'].iloc[-1]:.2f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nIssue-Specific Findings:\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        for issue in analyzer.unique_issues:\n",
    "            if issue in analyzer.polarization_dfs:\n",
    "                issue_name = analyzer.ISSUE_MAP[issue]\n",
    "                df = analyzer.polarization_dfs[issue]\n",
    "                f.write(f\"\\n{issue_name}:\\n\")\n",
    "                f.write(f\"- Polarization Change: {df['political_gap'].iloc[-1] - df['political_gap'].iloc[0]:.2f}\\n\")\n",
    "                f.write(f\"- Emotional Intensity Change: {df['emotional_gap'].iloc[-1] - df['emotional_gap'].iloc[0]:.2f}\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'analysis_results' directory.\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"1. analysis_results/plots/ - Visualization plots\")\n",
    "    print(\"2. analysis_results/trend_data.json - Raw trend data\")\n",
    "    print(\"3. analysis_results/summary_report.txt - Analysis summary\")\n",
    "\n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_analyzer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_heatmaps(analyzer, save_dir='analysis_results/plots/issue_heatmaps'):\n",
    "    \"\"\"Recreate heatmaps using all congresses\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare data for heatmaps - Force numerical sorting of congresses\n",
    "    congresses = sorted(analyzer.analysis_results.keys(), key=int)  # This is the key change\n",
    "    issues = list(analyzer.ISSUE_MAP.values())\n",
    "    \n",
    "    # Initialize matrices for different metrics\n",
    "    prevalence_matrix = np.zeros((len(issues), len(congresses)))\n",
    "    emotional_matrix = np.zeros((len(issues), len(congresses)))\n",
    "    political_matrix = np.zeros((len(issues), len(congresses)))\n",
    "    \n",
    "    # Fill matrices\n",
    "    for i, issue in enumerate(issues):\n",
    "        for j, congress in enumerate(congresses):\n",
    "            df = analyzer.analysis_results[congress]\n",
    "            issue_label = [k for k, v in analyzer.ISSUE_MAP.items() if v == issue][0]\n",
    "            issue_mask = df['issues'].apply(lambda x: issue_label in x)\n",
    "            issue_df = df[issue_mask]\n",
    "            \n",
    "            if not issue_df.empty:\n",
    "                prevalence_matrix[i, j] = len(issue_df) / len(df) * 100\n",
    "                emotional_matrix[i, j] = issue_df['emotional_intensity'].mean()\n",
    "                political_matrix[i, j] = issue_df['political_spectrum'].mean()\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.rcParams['figure.figsize'] = (20, 10)\n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    \n",
    "    # Plot heatmaps with proper congress ordering\n",
    "    plt.figure()\n",
    "    sns.heatmap(prevalence_matrix, \n",
    "                xticklabels=congresses,  # These will now be in numerical order\n",
    "                yticklabels=issues,\n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='.1f')\n",
    "    plt.title('Issue Prevalence Over Time (%)')\n",
    "    plt.xlabel('Congress')\n",
    "    plt.ylabel('Issue')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/issue_prevalence.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(emotional_matrix,\n",
    "                xticklabels=congresses,\n",
    "                yticklabels=issues,\n",
    "                cmap='RdBu_r',\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                vmin=1, vmax=5)\n",
    "    plt.title('Average Emotional Intensity by Issue Over Time')\n",
    "    plt.xlabel('Congress')\n",
    "    plt.ylabel('Issue')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/issue_emotional.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(political_matrix,\n",
    "                xticklabels=congresses,\n",
    "                yticklabels=issues,\n",
    "                cmap='RdBu_r',\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                vmin=1, vmax=5)\n",
    "    plt.title('Average Political Position by Issue Over Time')\n",
    "    plt.xlabel('Congress')\n",
    "    plt.ylabel('Issue')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/issue_political.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Run this with your existing analyzer object\n",
    "remake_heatmaps(output_analyzer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
