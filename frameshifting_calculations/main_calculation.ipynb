{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodri/Desktop/MIT Fall 2024 Classes/6.8611/Project/NLP-Frameshifting/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertForSequenceClassification, RobertaModel\n",
    "from datetime import datetime\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model for axis classification\n",
    "\n",
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Unfreeze more layers since we have more data\n",
    "        for param in self.roberta.encoder.layer[-8:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Shared features layer\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use mean pooling instead of just [CLS] token\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Get shared features\n",
    "        shared_features = self.shared_features(pooled_output)\n",
    "        \n",
    "        # Get task-specific predictions\n",
    "        emotional_logits = self.emotional_classifier(shared_features)\n",
    "        political_logits = self.political_classifier(shared_features)\n",
    "        \n",
    "        return emotional_logits, political_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speech_file(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load speeches from a single congress file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file containing speeches\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping speech IDs to speech texts\n",
    "    \"\"\"\n",
    "    speeches = {}\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            try:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) == 2:\n",
    "                    speech_id, speech = parts\n",
    "                    word_count = len(speech.split())\n",
    "                    if 35 < word_count < 400:\n",
    "                        speeches[speech_id] = speech\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return speeches\n",
    "\n",
    "def load_congress_data(congress_range: range, base_paths: Dict[str, str]) -> Dict[str, Dict[str, Dict]]:\n",
    "    \"\"\"\n",
    "    Load speeches and party information from multiple congresses\n",
    "    \n",
    "    Args:\n",
    "        congress_range: Range of congress numbers to load\n",
    "        base_paths: Dictionary mapping 'bound' and 'daily' to paths\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping congress numbers to dictionaries of speeches and party information\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for congress in tqdm(congress_range, desc=\"Loading congress data\"):\n",
    "        # Zero-pad congress number to 3 digits\n",
    "        congress_str = str(congress)\n",
    "        congress_str_padded = f\"{congress:03d}\"  # This will convert 79 to \"079\", 111 to \"111\", etc.\n",
    "        \n",
    "        # Determine which base path to use\n",
    "        if congress <= 111:\n",
    "            path = base_paths['bound']\n",
    "        else:\n",
    "            path = base_paths['daily']\n",
    "            \n",
    "        # Load speeches using padded number\n",
    "        speech_file = os.path.join(path, f\"speeches_{congress_str_padded}.txt\")\n",
    "        if not os.path.exists(speech_file):\n",
    "            print(f\"Could not find speech file: {speech_file}\")\n",
    "            continue\n",
    "            \n",
    "        # Read speeches\n",
    "        speeches = {}\n",
    "        with open(speech_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            next(file)  # Skip header\n",
    "            for line in file:\n",
    "                try:\n",
    "                    parts = line.strip().split('|')\n",
    "                    if len(parts) == 2:\n",
    "                        speech_id, speech = parts\n",
    "                        word_count = len(speech.split())\n",
    "                        if 35 < word_count < 400:\n",
    "                            speeches[speech_id] = {\"speech\": speech}\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # Load party information with padded number\n",
    "        speaker_map_file = os.path.join(path, f\"{congress_str_padded}_SpeakerMap.txt\")\n",
    "        if os.path.exists(speaker_map_file):\n",
    "            with open(speaker_map_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                header = file.readline().strip().split('|')\n",
    "                speech_id_idx = header.index('speech_id')\n",
    "                party_idx = header.index('party')\n",
    "                \n",
    "                for line in file:\n",
    "                    try:\n",
    "                        parts = line.strip().split('|')\n",
    "                        speech_id = parts[speech_id_idx]\n",
    "                        party = parts[party_idx]\n",
    "                        if speech_id in speeches:\n",
    "                            speeches[speech_id]['party'] = party\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        else:\n",
    "            print(f\"Could not find speaker map file: {speaker_map_file}\")\n",
    "        \n",
    "        # Only keep speeches with party information\n",
    "        speeches = {\n",
    "            k: v for k, v in speeches.items()\n",
    "            if 'party' in v and v['party'] in ['D', 'R']\n",
    "        }\n",
    "        \n",
    "        if speeches:\n",
    "            all_data[congress_str] = speeches\n",
    "            print(f\"Loaded {len(speeches)} speeches for congress {congress_str_padded}\")\n",
    "        else:\n",
    "            print(f\"No valid speeches found for congress {congress_str_padded}\")\n",
    "            \n",
    "    return all_data\n",
    "\n",
    "def load_party_data(congress_range: range, base_paths: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load party affiliations for speakers\n",
    "    \n",
    "    Args:\n",
    "        congress_range: Range of congress numbers to load\n",
    "        base_paths: Dictionary mapping 'bound' and 'daily' to paths\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping speech IDs to party affiliations\n",
    "    \"\"\"\n",
    "    party_data = {}\n",
    "    \n",
    "    for congress in congress_range:\n",
    "        congress_str = str(congress)\n",
    "        \n",
    "        # Determine which base path to use\n",
    "        if congress <= 111:\n",
    "            path = base_paths['bound']\n",
    "        else:\n",
    "            path = base_paths['daily']\n",
    "            \n",
    "        desc_file = os.path.join(path, f\"descr_{congress_str}.txt\")\n",
    "        \n",
    "        if os.path.exists(desc_file):\n",
    "            with open(desc_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                next(file)  # Skip header\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        parts = line.strip().split('|')\n",
    "                        if len(parts) >= 2:\n",
    "                            speech_id = parts[0]\n",
    "                            party = parts[-1]  # Party is usually the last column\n",
    "                            if party in ['D', 'R']:  # Only keep Democrat and Republican\n",
    "                                party_data[speech_id] = party\n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "    return party_data\n",
    "\n",
    "class CongressionalAnalysis:\n",
    "    def __init__(self, \n",
    "                issue_model_path: str,\n",
    "                axis_model_path: str,\n",
    "                congress_range: range = range(79, 115)):\n",
    "        \"\"\"Initialize the analysis pipeline\"\"\"\n",
    "        self.congress_range = congress_range\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.VALID_ISSUES = {\n",
    "            'Economy and Jobs',\n",
    "            'Health and Social Services',\n",
    "            'Education and Innovation',\n",
    "            'Environment and Energy',\n",
    "            'Defense and Security',\n",
    "            'Immigration and Border Policy',\n",
    "            'Justice and Civil Rights',\n",
    "            'Infrastructure and Transportation',\n",
    "            'Budget and Fiscal Responsibility'\n",
    "        }\n",
    "\n",
    "        self.ISSUE_MAP = {\n",
    "            'LABEL_21': 'Economy and Jobs',\n",
    "            'LABEL_31': 'Health and Social Services',\n",
    "            'LABEL_22': 'Education and Innovation',\n",
    "            'LABEL_26': 'Environment and Energy',\n",
    "            'LABEL_19': 'Defense and Security',\n",
    "            'LABEL_43': 'Immigration and Border Policy',\n",
    "            'LABEL_47': 'Justice and Civil Rights',\n",
    "            'LABEL_44': 'Infrastructure and Transportation',\n",
    "            'LABEL_8': 'Budget and Fiscal Responsibility'\n",
    "        }\n",
    "        \n",
    "        # Load models\n",
    "        print(\"Loading models...\")\n",
    "        self.issue_model = self.load_issue_model(issue_model_path)\n",
    "        self.axis_model = self.load_axis_model(axis_model_path)\n",
    "        \n",
    "        # Create unique_issues list\n",
    "        self.unique_issues = list(self.issue_model.config.id2label.values())\n",
    "        print(f\"Loaded {len(self.unique_issues)} unique issues\")\n",
    "        \n",
    "        # Load tokenizers\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(issue_model_path)\n",
    "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Initialize data structures\n",
    "        self.speeches = {}\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_issue_model(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Load the issue classification model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model directory\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch model for issue classification\n",
    "        \"\"\"\n",
    "        return BertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    def load_axis_model(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Load the axis prediction model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model directory\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch model for axis prediction\n",
    "        \"\"\"\n",
    "        model_state = torch.load(model_path, map_location=self.device)\n",
    "        model = PoliticalSpeechClassifier()\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        return model\n",
    "    \n",
    "    def load_data(self, base_paths: Dict[str, str], sample_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Load and sample speeches from each congress\n",
    "        \n",
    "        Args:\n",
    "            base_paths: Dictionary mapping 'bound' and 'daily' to paths\n",
    "            sample_size: Number of speeches to sample from each congress\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(\"Loading congress data...\")\n",
    "        all_data = load_congress_data(self.congress_range, base_paths)\n",
    "        \n",
    "        # Sample speeches from each congress\n",
    "        print(\"Sampling speeches...\")\n",
    "        for congress, speeches in all_data.items():\n",
    "            # Split by party\n",
    "            dem_speeches = {k: v for k, v in speeches.items() if v['party'] == 'D'}\n",
    "            rep_speeches = {k: v for k, v in speeches.items() if v['party'] == 'R'}\n",
    "            \n",
    "            per_party = sample_size // 2\n",
    "            sampled_speeches = {}\n",
    "            \n",
    "            # Only proceed if we have enough speeches from both parties\n",
    "            if len(dem_speeches) >= per_party and len(rep_speeches) >= per_party:\n",
    "                dem_items = list(dem_speeches.items())\n",
    "                rep_items = list(rep_speeches.items())\n",
    "                \n",
    "                # Sample equally from each party\n",
    "                dem_sample = dict(random.sample(dem_items, per_party))\n",
    "                rep_sample = dict(random.sample(rep_items, per_party))\n",
    "                \n",
    "                sampled_speeches.update(dem_sample)\n",
    "                sampled_speeches.update(rep_sample)\n",
    "                \n",
    "                self.speeches[congress] = sampled_speeches\n",
    "            else:\n",
    "                print(f\"Warning: Not enough speeches from both parties in congress {congress}\")\n",
    "                print(f\"Democratic speeches: {len(dem_speeches)}\")\n",
    "                print(f\"Republican speeches: {len(rep_speeches)}\")\n",
    "\n",
    "    def analyze_speeches(self):\n",
    "        \"\"\"\n",
    "        Analyze all loaded speeches using both models\n",
    "        \"\"\"\n",
    "        print(\"Analyzing speeches...\")\n",
    "        self.issue_model.to(self.device)\n",
    "        self.axis_model.to(self.device)\n",
    "        self.issue_model.eval()\n",
    "        self.axis_model.eval()\n",
    "\n",
    "        # Create unique_issues list\n",
    "        self.unique_issues = list(self.issue_model.config.id2label.values())\n",
    "\n",
    "        for congress, speeches in tqdm(self.speeches.items(), desc=\"Processing congresses\"):\n",
    "            congress_results = []\n",
    "\n",
    "            # print length of speeches\n",
    "            print(f\"Number of speeches in congress {congress}: {len(speeches)}\")\n",
    "            \n",
    "            for speech_id, speech_data in tqdm(speeches.items(), desc=\"Processing speeches\"):\n",
    "                try:\n",
    "                    # Extract data correctly from the speech_data dictionary\n",
    "                    speech_text = speech_data['speech']\n",
    "                    party = speech_data['party']\n",
    "                    \n",
    "                    if not speech_text or not party:\n",
    "                        continue\n",
    "\n",
    "                    # Predict issues\n",
    "                    issues = self.predict_issues(speech_text)\n",
    "                    \n",
    "                    # Predict axis scores\n",
    "                    axis_scores = self.predict_axis_scores(speech_text)\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'congress': int(congress),\n",
    "                        'speech_id': speech_id,\n",
    "                        'party': party,\n",
    "                        'issues': issues,\n",
    "                        'emotional_intensity': axis_scores['emotional_intensity'],\n",
    "                        'political_spectrum': axis_scores['political_spectrum'],\n",
    "                        'emotional_confidence': axis_scores['emotional_confidence'],\n",
    "                        'political_confidence': axis_scores['political_confidence']\n",
    "                    }\n",
    "                    \n",
    "                    congress_results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing speech {speech_id}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if congress_results:\n",
    "                self.analysis_results[congress] = pd.DataFrame(congress_results).fillna('')\n",
    "                \n",
    "                # print to verify data\n",
    "                print(f\"\\nCongress {congress} results:\")\n",
    "                print(f\"Number of speeches processed: {len(congress_results)}\")\n",
    "                print(\"Columns:\", self.analysis_results[congress].columns.tolist())\n",
    "                print(\"Party distribution:\", self.analysis_results[congress]['party'].value_counts())\n",
    "    \n",
    "    def predict_issues(self, speech_text: str, threshold: float = 0.5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict issues with mapping to standard names\n",
    "        \n",
    "        Args:\n",
    "            speech_text: Text of the speech\n",
    "            threshold: Minimum probability to consider an issue\n",
    "\n",
    "        Returns:\n",
    "            List of predicted issues for the speech\n",
    "        \"\"\"\n",
    "        encoding = self.bert_tokenizer(\n",
    "            speech_text,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "            outputs = self.issue_model(**inputs)\n",
    "            probabilities = torch.sigmoid(outputs.logits).cpu().numpy().flatten()\n",
    "            \n",
    "            # Get predictions and map to standard names\n",
    "            raw_predictions = [\n",
    "                self.issue_model.config.id2label[i]\n",
    "                for i, prob in enumerate(probabilities)\n",
    "                if prob >= threshold\n",
    "            ]\n",
    "            \n",
    "            # Filter to only valid issues\n",
    "            valid_predictions = [\n",
    "                issue for issue in raw_predictions\n",
    "                if issue in self.ISSUE_MAP\n",
    "            ]\n",
    "            \n",
    "            return valid_predictions\n",
    "    \n",
    "    def predict_axis_scores(self, speech_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict axis scores for a single speech\n",
    "        \n",
    "        Args:\n",
    "            speech_text: Text of the speech\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with predicted emotional intensity and political spectrum scores\n",
    "            of the speech\n",
    "        \"\"\"\n",
    "        encoding = self.roberta_tokenizer(\n",
    "            speech_text,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "            emotional_logits, political_logits = self.axis_model(**inputs)\n",
    "            \n",
    "            emotional_probs = torch.softmax(emotional_logits, dim=1)\n",
    "            political_probs = torch.softmax(political_logits, dim=1)\n",
    "            \n",
    "            emotional_pred = torch.argmax(emotional_probs, dim=1).item() + 1\n",
    "            political_pred = torch.argmax(political_probs, dim=1).item() + 1\n",
    "            \n",
    "            emotional_conf = emotional_probs[0][emotional_pred-1].item()\n",
    "            political_conf = political_probs[0][political_pred-1].item()\n",
    "        \n",
    "        return {\n",
    "            'emotional_intensity': emotional_pred,\n",
    "            'emotional_confidence': emotional_conf,\n",
    "            'political_spectrum': political_pred,\n",
    "            'political_confidence': political_conf\n",
    "        }\n",
    "    \n",
    "    def analyze_framing_shifts(self, save_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        Analyze framing shifts over time\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save detailed metrics\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(\"Analyzing framing shifts...\")\n",
    "\n",
    "        self.trends = {\n",
    "            'by_party': defaultdict(list),\n",
    "            'by_issue': defaultdict(list),\n",
    "            'by_party_issue': defaultdict(lambda: defaultdict(list))\n",
    "        }\n",
    "\n",
    "        # Initialize metrics tracking\n",
    "        self.metrics = {\n",
    "            'global': {\n",
    "                'emotional_volatility': [],\n",
    "                'political_volatility': [],\n",
    "                'party_divergence': []\n",
    "            },\n",
    "            'by_party': defaultdict(lambda: {\n",
    "                'emotional_trend': [],\n",
    "                'political_trend': []\n",
    "            })\n",
    "        }\n",
    "\n",
    "        # Process each congress\n",
    "        for congress, df in self.analysis_results.items():\n",
    "            if 'party' not in df.columns:\n",
    "                print(f\"Warning: No party information for congress {congress}\")\n",
    "                continue\n",
    "            congress_num = int(congress)\n",
    "            \n",
    "            # Analyze by party\n",
    "            for party in ['D', 'R']:\n",
    "                party_df = df[df['party'] == party]\n",
    "                if not party_df.empty:\n",
    "                    self.trends['by_party'][party].append({\n",
    "                        'congress': congress_num,\n",
    "                        'emotional_avg': party_df['emotional_intensity'].mean(),\n",
    "                        'emotional_std': party_df['emotional_intensity'].std(),\n",
    "                        'political_avg': party_df['political_spectrum'].mean(),\n",
    "                        'political_std': party_df['political_spectrum'].std(),\n",
    "                        'count': len(party_df)\n",
    "                    })\n",
    "\n",
    "                    self.metrics['by_party'][party]['emotional_trend'].append({\n",
    "                        'congress': congress_num,\n",
    "                        'mean': party_df['emotional_intensity'].mean(),\n",
    "                        'std': party_df['emotional_intensity'].std()\n",
    "                    })\n",
    "            \n",
    "            # Analyze by issue\n",
    "            for issue in self.unique_issues:\n",
    "                if issue in self.ISSUE_MAP:\n",
    "                    issue_mask = df['issues'].apply(lambda x: issue in x)\n",
    "                    issue_df = df[issue_mask]\n",
    "                    if not issue_df.empty:\n",
    "                        self.trends['by_issue'][issue].append({\n",
    "                            'congress': congress_num,\n",
    "                            'emotional_avg': issue_df['emotional_intensity'].mean(),\n",
    "                            'emotional_std': issue_df['emotional_intensity'].std(),\n",
    "                            'political_avg': issue_df['political_spectrum'].mean(),\n",
    "                            'political_std': issue_df['political_spectrum'].std(),\n",
    "                            'count': len(issue_df)\n",
    "                        })\n",
    "                        \n",
    "                        # Analyze by party within issue\n",
    "                        for party in ['D', 'R']:\n",
    "                            party_issue_df = issue_df[issue_df['party'] == party]\n",
    "                            if not party_issue_df.empty:\n",
    "                                self.trends['by_party_issue'][issue][party].append({\n",
    "                                    'congress': congress_num,\n",
    "                                    'emotional_avg': party_issue_df['emotional_intensity'].mean(),\n",
    "                                    'emotional_std': party_issue_df['emotional_intensity'].std(),\n",
    "                                    'political_avg': party_issue_df['political_spectrum'].mean(),\n",
    "                                    'political_std': party_issue_df['political_spectrum'].std(),\n",
    "                                    'count': len(party_issue_df)\n",
    "                                })\n",
    "            \n",
    "            # Add volatility metrics\n",
    "            self.metrics['global']['emotional_volatility'].append({\n",
    "                'congress': congress_num,\n",
    "                'std': df['emotional_intensity'].std()\n",
    "            })\n",
    "            self.metrics['global']['political_volatility'].append({\n",
    "                'congress': congress_num,\n",
    "                'std': df['political_spectrum'].std()\n",
    "            })\n",
    "\n",
    "        # Convert trend data to DataFrames\n",
    "        self.trend_dfs = {\n",
    "            'by_party': {\n",
    "                party: pd.DataFrame(data)\n",
    "                for party, data in self.trends['by_party'].items()\n",
    "            },\n",
    "            'by_issue': {\n",
    "                issue: pd.DataFrame(data)\n",
    "                for issue, data in self.trends['by_issue'].items()\n",
    "            },\n",
    "            'by_party_issue': {\n",
    "                issue: {\n",
    "                    party: pd.DataFrame(data)\n",
    "                    for party, data in party_data.items()\n",
    "                }\n",
    "                for issue, party_data in self.trends['by_party_issue'].items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save enhanced metrics\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(os.path.join(save_dir, 'detailed_metrics.json'), 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "\n",
    "    def calculate_polarization_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate polarization metrics over time\n",
    "        \"\"\"\n",
    "        print(\"Calculating polarization metrics...\")\n",
    "        \n",
    "        self.polarization_metrics = defaultdict(list)\n",
    "        \n",
    "        # Overall polarization\n",
    "        for congress in sorted(self.analysis_results.keys()):\n",
    "            df = self.analysis_results[congress]\n",
    "            dem_df = df[df['party'] == 'D']\n",
    "            rep_df = df[df['party'] == 'R']\n",
    "            \n",
    "            metrics = {\n",
    "                'congress': int(congress),\n",
    "                'emotional_gap': (rep_df['emotional_intensity'].mean() - \n",
    "                                dem_df['emotional_intensity'].mean()),\n",
    "                'political_gap': (rep_df['political_spectrum'].mean() - \n",
    "                                dem_df['political_spectrum'].mean()),\n",
    "                'emotional_overlap': self._calculate_distribution_overlap(\n",
    "                    dem_df['emotional_intensity'], rep_df['emotional_intensity']\n",
    "                ),\n",
    "                'political_overlap': self._calculate_distribution_overlap(\n",
    "                    dem_df['political_spectrum'], rep_df['political_spectrum']\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            self.polarization_metrics['overall'].append(metrics)\n",
    "        \n",
    "        # By issue polarization\n",
    "        for issue in self.unique_issues:\n",
    "            if issue in self.ISSUE_MAP:\n",
    "                for congress in sorted(self.analysis_results.keys()):\n",
    "                    df = self.analysis_results[congress]\n",
    "                    issue_mask = df['issues'].apply(lambda x: issue in x)\n",
    "                    issue_df = df[issue_mask]\n",
    "                    \n",
    "                    if len(issue_df) > 10:  # Only calculate if enough samples\n",
    "                        dem_df = issue_df[issue_df['party'] == 'D']\n",
    "                        rep_df = issue_df[issue_df['party'] == 'R']\n",
    "                        \n",
    "                        if len(dem_df) > 5 and len(rep_df) > 5:\n",
    "                            metrics = {\n",
    "                                'congress': int(congress),\n",
    "                                'emotional_gap': (rep_df['emotional_intensity'].mean() - \n",
    "                                                dem_df['emotional_intensity'].mean()),\n",
    "                                'political_gap': (rep_df['political_spectrum'].mean() - \n",
    "                                                dem_df['political_spectrum'].mean()),\n",
    "                                'emotional_overlap': self._calculate_distribution_overlap(\n",
    "                                    dem_df['emotional_intensity'], rep_df['emotional_intensity']\n",
    "                                ),\n",
    "                                'political_overlap': self._calculate_distribution_overlap(\n",
    "                                    dem_df['political_spectrum'], rep_df['political_spectrum']\n",
    "                                )\n",
    "                            }\n",
    "                            \n",
    "                            self.polarization_metrics[issue].append(metrics)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        self.polarization_dfs = {\n",
    "            key: pd.DataFrame(data)\n",
    "            for key, data in self.polarization_metrics.items()\n",
    "        }\n",
    "    \n",
    "    def _calculate_distribution_overlap(self, dist1, dist2):\n",
    "        \"\"\"\n",
    "        Calculate overlap between two distributions\n",
    "        \n",
    "        Args:\n",
    "            dist1: First distribution\n",
    "            dist2: Second distribution\n",
    "\n",
    "        Returns:\n",
    "            Overlap between the two distributions\n",
    "        \"\"\"\n",
    "        hist1, bins = np.histogram(dist1, bins=5, density=True)\n",
    "        hist2, _ = np.histogram(dist2, bins=bins, density=True)\n",
    "        return np.minimum(hist1, hist2).sum() * (bins[1] - bins[0])\n",
    "\n",
    "    def _plot_issue_heatmaps(self, save_dir):\n",
    "        \"\"\"\n",
    "        Create heatmaps showing issue prevalence and characteristics over time\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save the heatmaps\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Create directory for issue heatmaps\n",
    "        heatmap_dir = os.path.join(save_dir, 'issue_heatmaps')\n",
    "        os.makedirs(heatmap_dir, exist_ok=True)\n",
    "        \n",
    "        # Prepare data\n",
    "        congresses = sorted(self.analysis_results.keys(), key=int)\n",
    "        issues = list(self.ISSUE_MAP.values())\n",
    "        \n",
    "        # Initialize matrices for different metrics\n",
    "        prevalence_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        emotional_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        political_matrix = np.zeros((len(issues), len(congresses)))\n",
    "        \n",
    "        # Fill matrices\n",
    "        for i, issue in enumerate(issues):\n",
    "            for j, congress in enumerate(congresses):\n",
    "                df = self.analysis_results[congress]\n",
    "                issue_label = [k for k, v in self.ISSUE_MAP.items() if v == issue][0]\n",
    "                issue_mask = df['issues'].apply(lambda x: issue_label in x)\n",
    "                issue_df = df[issue_mask]\n",
    "                \n",
    "                if not issue_df.empty:\n",
    "                    prevalence_matrix[i, j] = len(issue_df) / len(df) * 100\n",
    "                    emotional_matrix[i, j] = issue_df['emotional_intensity'].mean()\n",
    "                    political_matrix[i, j] = issue_df['political_spectrum'].mean()\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(prevalence_matrix, \n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='YlOrRd',\n",
    "                    annot=True,\n",
    "                    fmt='.1f')\n",
    "        plt.title('Issue Prevalence Over Time (%)')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_prevalence.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(emotional_matrix,\n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='RdBu_r',\n",
    "                    annot=True,\n",
    "                    fmt='.2f',\n",
    "                    vmin=1, vmax=5)\n",
    "        plt.title('Average Emotional Intensity by Issue Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_emotional.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(political_matrix,\n",
    "                    xticklabels=congresses,\n",
    "                    yticklabels=issues,\n",
    "                    cmap='RdBu_r',\n",
    "                    annot=True,\n",
    "                    fmt='.2f',\n",
    "                    vmin=1, vmax=5)\n",
    "        plt.title('Average Political Position by Issue Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Issue')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{heatmap_dir}/issue_political.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def calculate_issue_dynamics(self):\n",
    "        \"\"\"\n",
    "        Calculate how issues change over time\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with issue dynamics metrics\n",
    "        \"\"\"\n",
    "        dynamics = {}\n",
    "        \n",
    "        for issue in self.ISSUE_MAP.values():\n",
    "            dynamics[issue] = {\n",
    "                'volatility': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                },\n",
    "                'trend': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                },\n",
    "                'party_gap': {\n",
    "                    'emotional': [],\n",
    "                    'political': []\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Calculate metrics per congress\n",
    "            for congress in sorted(self.analysis_results.keys()):\n",
    "                df = self.analysis_results[congress]\n",
    "                issue_label = [k for k, v in self.ISSUE_MAP.items() if v == issue][0]\n",
    "                issue_mask = df['issues'].apply(lambda x: issue_label in x)\n",
    "                issue_df = df[issue_mask]\n",
    "                \n",
    "                if len(issue_df) > 10:  # Only calculate if enough samples\n",
    "                    # Volatility (standard deviation)\n",
    "                    dynamics[issue]['volatility']['emotional'].append(\n",
    "                        issue_df['emotional_intensity'].std()\n",
    "                    )\n",
    "                    dynamics[issue]['volatility']['political'].append(\n",
    "                        issue_df['political_spectrum'].std()\n",
    "                    )\n",
    "                    \n",
    "                    # Party differences\n",
    "                    dem_df = issue_df[issue_df['party'] == 'D']\n",
    "                    rep_df = issue_df[issue_df['party'] == 'R']\n",
    "                    \n",
    "                    if len(dem_df) > 5 and len(rep_df) > 5:\n",
    "                        dynamics[issue]['party_gap']['emotional'].append(\n",
    "                            rep_df['emotional_intensity'].mean() - dem_df['emotional_intensity'].mean()\n",
    "                        )\n",
    "                        dynamics[issue]['party_gap']['political'].append(\n",
    "                            rep_df['political_spectrum'].mean() - dem_df['political_spectrum'].mean()\n",
    "                        )\n",
    "        \n",
    "        return dynamics\n",
    "    \n",
    "    def plot_framing_trends(self, save_dir='plots'):\n",
    "        \"\"\"\n",
    "        Generate plots for framing trends\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save the plots\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Plot overall party trends\n",
    "        self._plot_party_trends(save_dir)\n",
    "        \n",
    "        # Plot issue-specific trends\n",
    "        self._plot_issue_trends(save_dir)\n",
    "        \n",
    "        # Plot polarization trends\n",
    "        self._plot_polarization_trends(save_dir)\n",
    "\n",
    "        # New issue analysis plots\n",
    "        self._plot_issue_heatmaps(save_dir)\n",
    "\n",
    "        # Calculate and save issue dynamics\n",
    "        dynamics = self.calculate_issue_dynamics()\n",
    "        with open(os.path.join(save_dir, 'issue_dynamics.json'), 'w') as f:\n",
    "            json.dump(dynamics, f, indent=2)\n",
    "    \n",
    "    def _plot_party_trends(self, save_dir):\n",
    "        \"\"\"Plot party-level trends with standardized scales\"\"\"\n",
    "        # Emotional Intensity by Party\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for party in ['D', 'R']:\n",
    "            df = self.trend_dfs['by_party'][party]\n",
    "            plt.plot(df['congress'], df['emotional_avg'], \n",
    "                    label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "            plt.fill_between(df['congress'],\n",
    "                            df['emotional_avg'] - df['emotional_std'],\n",
    "                            df['emotional_avg'] + df['emotional_std'],\n",
    "                            alpha=0.2)\n",
    "        \n",
    "        plt.title('Emotional Intensity by Party Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Average Emotional Intensity')\n",
    "        plt.ylim(1, 5)  # fixed scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/emotional_intensity_by_party.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Political Spectrum by Party\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for party in ['D', 'R']:\n",
    "            df = self.trend_dfs['by_party'][party]\n",
    "            plt.plot(df['congress'], df['political_avg'],\n",
    "                    label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "            plt.fill_between(df['congress'],\n",
    "                            df['political_avg'] - df['political_std'],\n",
    "                            df['political_avg'] + df['political_std'],\n",
    "                            alpha=0.2)\n",
    "        \n",
    "        plt.title('Political Spectrum Position by Party Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Average Political Spectrum Position')\n",
    "        plt.ylim(1, 5)  # fixed scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/political_spectrum_by_party.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_issue_trends(self, save_dir):\n",
    "        \"\"\"Plot issue-level trends\"\"\"\n",
    "        # Skip empty or invalid issues\n",
    "        if not self.trends['by_issue']:\n",
    "            return\n",
    "\n",
    "        for issue in self.ISSUE_MAP.values():  # Use defined issue map\n",
    "            # find the key in self.ISSUE_MAP that corresponds to the issue\n",
    "            for key, value in self.ISSUE_MAP.items():\n",
    "                if value == issue:\n",
    "                    issue_key = key\n",
    "                    break\n",
    "            if issue_key not in self.trend_dfs['by_issue']:\n",
    "                print(f\"Skipping issue: {issue_key}, whose real name is {issue}\")\n",
    "                print(self.trend_dfs['by_issue'].keys())\n",
    "                continue\n",
    "                \n",
    "            # Create directory for issue-specific plots\n",
    "            issue_dir = os.path.join(save_dir, 'issues', issue.lower().replace(' ', '_'))\n",
    "            os.makedirs(issue_dir, exist_ok=True)\n",
    "            \n",
    "            # Emotional Intensity\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for party in ['D', 'R']:\n",
    "                if issue_key in self.trend_dfs['by_party_issue'] and \\\n",
    "                party in self.trend_dfs['by_party_issue'][issue_key]:\n",
    "                    df = self.trend_dfs['by_party_issue'][issue_key][party]\n",
    "                    plt.plot(df['congress'], df['emotional_avg'],\n",
    "                            label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "                    plt.fill_between(df['congress'],\n",
    "                                df['emotional_avg'] - df['emotional_std'],\n",
    "                                df['emotional_avg'] + df['emotional_std'],\n",
    "                                alpha=0.2)\n",
    "            \n",
    "            plt.title(f'Emotional Intensity Over Time: {issue}')\n",
    "            plt.xlabel('Congress')\n",
    "            plt.ylabel('Average Emotional Intensity')\n",
    "            plt.ylim(1, 5)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{issue_dir}/emotional_intensity.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Political Spectrum\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for party in ['D', 'R']:\n",
    "                if issue_key in self.trend_dfs['by_party_issue'] and \\\n",
    "                party in self.trend_dfs['by_party_issue'][issue_key]:\n",
    "                    df = self.trend_dfs['by_party_issue'][issue_key][party]\n",
    "                    plt.plot(df['congress'], df['political_avg'],\n",
    "                            label=f\"{'Democratic' if party == 'D' else 'Republican'}\")\n",
    "                    plt.fill_between(df['congress'],\n",
    "                                df['political_avg'] - df['political_std'],\n",
    "                                df['political_avg'] + df['political_std'],\n",
    "                                alpha=0.2)\n",
    "            \n",
    "            plt.title(f'Political Spectrum Position Over Time: {issue}')\n",
    "            plt.xlabel('Congress')\n",
    "            plt.ylabel('Average Political Spectrum Position')\n",
    "            plt.ylim(1, 5)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{issue_dir}/political_spectrum.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    def _plot_polarization_trends(self, save_dir):\n",
    "        \"\"\"Plot polarization trends with standardized scales\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df = self.polarization_dfs['overall']\n",
    "        plt.plot(df['congress'], df['emotional_gap'], label='Emotional Gap')\n",
    "        plt.plot(df['congress'], df['political_gap'], label='Political Gap')\n",
    "        plt.title('Party Polarization Over Time')\n",
    "        plt.xlabel('Congress')\n",
    "        plt.ylabel('Party Gap')\n",
    "        plt.ylim(-4, 4)  # Maximum possible gap is ±4 on a 1-5 scale\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{save_dir}/overall_polarization.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def save_complete_dataset(self, save_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        Save all raw data and computed metrics needed to recreate plots\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save the dataset\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(\"\\nSaving complete dataset...\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save raw speech analysis results\n",
    "        raw_data = {}\n",
    "        for congress, df in self.analysis_results.items():\n",
    "            raw_data[congress] = df.to_dict('records')\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'raw_speech_data.json'), 'w') as f:\n",
    "            json.dump(raw_data, f, indent=2)\n",
    "        \n",
    "        # Save trend data with standardized issue names\n",
    "        trend_data = {\n",
    "            'by_party': {\n",
    "                party: df.to_dict('records')\n",
    "                for party, df in self.trend_dfs['by_party'].items()\n",
    "            },\n",
    "            'by_issue': {\n",
    "                self.ISSUE_MAP.get(issue, issue): df.to_dict('records')\n",
    "                for issue, df in self.trend_dfs['by_issue'].items()\n",
    "            },\n",
    "            'by_party_issue': {\n",
    "                self.ISSUE_MAP.get(issue, issue): {\n",
    "                    party: df.to_dict('records')\n",
    "                    for party, df in party_data.items()\n",
    "                }\n",
    "                for issue, party_data in self.trend_dfs['by_party_issue'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'trend_data.json'), 'w') as f:\n",
    "            json.dump(trend_data, f, indent=2)\n",
    "        \n",
    "        # Save polarization data\n",
    "        polarization_data = {\n",
    "            self.ISSUE_MAP.get(issue, issue): df.to_dict('records')\n",
    "            for issue, df in self.polarization_dfs.items()\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'polarization_data.json'), 'w') as f:\n",
    "            json.dump(polarization_data, f, indent=2)\n",
    "        \n",
    "        # Save a metadata file with mapping information\n",
    "        metadata = {\n",
    "            'issue_map': self.ISSUE_MAP,\n",
    "            'valid_issues': list(self.VALID_ISSUES),\n",
    "            'congress_range': list(self.congress_range),\n",
    "            'data_format': {\n",
    "                'raw_speech_data': 'Dictionary mapping congress number to list of speech analysis results',\n",
    "                'trend_data': 'Contains by_party, by_issue, and by_party_issue trend calculations',\n",
    "                'polarization_data': 'Dictionary mapping issues to polarization metrics over time'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(\"Complete dataset saved successfully!\")\n",
    "        print(f\"Files created in {save_dir}:\")\n",
    "        print(\"1. raw_speech_data.json - Complete speech-level analysis results\")\n",
    "        print(\"2. trend_data.json - Aggregated trends by party and issue\")\n",
    "        print(\"3. polarization_data.json - Polarization metrics over time\")\n",
    "        print(\"4. metadata.json - Data format documentation and mappings\")\n",
    "\n",
    "    def save_extended_dataset(self, save_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        Save additional metrics and calculations\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save the extended dataset\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(\"\\nSaving extended dataset...\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_data = {\n",
    "            'global': {\n",
    "                'emotional_volatility': [\n",
    "                    {\n",
    "                        'congress': x['congress'],\n",
    "                        'std': x['std']\n",
    "                    } for x in self.metrics['global']['emotional_volatility']\n",
    "                ],\n",
    "                'political_volatility': [\n",
    "                    {\n",
    "                        'congress': x['congress'],\n",
    "                        'std': x['std']\n",
    "                    } for x in self.metrics['global']['political_volatility']\n",
    "                ],\n",
    "                'party_divergence': self.metrics['global']['party_divergence']\n",
    "            },\n",
    "            'by_party': {\n",
    "                party: {\n",
    "                    'emotional_trend': [\n",
    "                        {\n",
    "                            'congress': x['congress'],\n",
    "                            'mean': x['mean'],\n",
    "                            'std': x['std']\n",
    "                        } for x in data['emotional_trend']\n",
    "                    ]\n",
    "                } for party, data in self.metrics['by_party'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'extended_metrics.json'), 'w') as f:\n",
    "            json.dump(metrics_data, f, indent=2)\n",
    "        \n",
    "        # Save issue dynamics\n",
    "        dynamics = self.calculate_issue_dynamics()  # This was created but not saved in original\n",
    "        with open(os.path.join(save_dir, 'issue_dynamics.json'), 'w') as f:\n",
    "            json.dump(dynamics, f, indent=2)\n",
    "        \n",
    "        # Save run configuration and parameters\n",
    "        config_data = {\n",
    "            'congress_range': list(self.congress_range),\n",
    "            'issue_map': self.ISSUE_MAP,\n",
    "            'valid_issues': list(self.VALID_ISSUES),\n",
    "            'run_timestamp': datetime.now().isoformat(),\n",
    "            'device_used': str(self.device),\n",
    "            'model_configurations': {\n",
    "                'issue_model': {\n",
    "                    'type': 'BertForSequenceClassification',\n",
    "                    'num_labels': len(self.ISSUE_MAP)\n",
    "                },\n",
    "                'axis_model': {\n",
    "                    'type': 'PoliticalSpeechClassifier',\n",
    "                    'num_classes': 5\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'run_configuration.json'), 'w') as f:\n",
    "            json.dump(config_data, f, indent=2)\n",
    "            \n",
    "        print(\"Extended dataset saved successfully!\")\n",
    "        print(f\"Additional files created in {save_dir}:\")\n",
    "        print(\"1. extended_metrics.json - Detailed metrics and volatility measures\")\n",
    "        print(\"2. issue_dynamics.json - Issue-specific dynamics and trends\")\n",
    "        print(\"3. run_configuration.json - Run parameters and configuration\")\n",
    "\n",
    "    def save_all_data(self, save_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        Save everything from the analysis run\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save all data\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save all previously implemented data\n",
    "        self.save_complete_dataset()\n",
    "        self.save_extended_dataset()\n",
    "        \n",
    "        # Save raw trends data\n",
    "        raw_trends = {\n",
    "            'by_party': self.trends['by_party'],\n",
    "            'by_issue': self.trends['by_issue'],\n",
    "            'by_party_issue': self.trends['by_party_issue']\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'raw_trends.json'), 'w') as f:\n",
    "            json.dump(raw_trends, f, indent=2)\n",
    "        \n",
    "        print(\"\\nSaved raw trends data to raw_trends.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing analysis pipeline...\n",
      "Using device: cpu\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/xhv26dgs4tscxfcc4mdbh8zw0000gn/T/ipykernel_83298/3516383920.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(model_path, map_location=self.device)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74 unique issues\n",
      "\n",
      "Loading congressional data...\n",
      "Loading congress data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   3%|▎         | 1/36 [00:01<00:45,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68687 speeches for congress 079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   6%|▌         | 2/36 [00:02<00:38,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55732 speeches for congress 080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:   8%|▊         | 3/36 [00:03<00:41,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 91333 speeches for congress 081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  11%|█         | 4/36 [00:04<00:38,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71327 speeches for congress 082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  14%|█▍        | 5/36 [00:06<00:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71976 speeches for congress 083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  17%|█▋        | 6/36 [00:07<00:37,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56304 speeches for congress 084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  19%|█▉        | 7/36 [00:09<00:39,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 76595 speeches for congress 085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  22%|██▏       | 8/36 [00:10<00:41,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 77945 speeches for congress 086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  25%|██▌       | 9/36 [00:12<00:42,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78954 speeches for congress 087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  28%|██▊       | 10/36 [00:14<00:42,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82647 speeches for congress 088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  31%|███       | 11/36 [00:16<00:43,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82540 speeches for congress 089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  33%|███▎      | 12/36 [00:18<00:43,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89339 speeches for congress 090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  36%|███▌      | 13/36 [00:20<00:44,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 93230 speeches for congress 091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  39%|███▉      | 14/36 [00:22<00:42,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 79993 speeches for congress 092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  42%|████▏     | 15/36 [00:24<00:42,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90806 speeches for congress 093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  44%|████▍     | 16/36 [00:26<00:41,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 94878 speeches for congress 094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  47%|████▋     | 17/36 [00:29<00:39,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99770 speeches for congress 095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  50%|█████     | 18/36 [00:31<00:37,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 88242 speeches for congress 096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  53%|█████▎    | 19/36 [00:32<00:34,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71751 speeches for congress 097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  56%|█████▌    | 20/36 [00:34<00:31,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 73694 speeches for congress 098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  58%|█████▊    | 21/36 [00:36<00:29,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74726 speeches for congress 099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  61%|██████    | 22/36 [00:38<00:27,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68877 speeches for congress 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  64%|██████▍   | 23/36 [00:40<00:24,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61333 speeches for congress 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  67%|██████▋   | 24/36 [00:42<00:22,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59940 speeches for congress 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  69%|██████▉   | 25/36 [00:44<00:20,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60331 speeches for congress 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  72%|███████▏  | 26/36 [00:46<00:19,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 72521 speeches for congress 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  75%|███████▌  | 27/36 [00:47<00:16,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50320 speeches for congress 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  78%|███████▊  | 28/36 [00:49<00:14,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50657 speeches for congress 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  81%|████████  | 29/36 [00:51<00:12,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41234 speeches for congress 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  83%|████████▎ | 30/36 [00:52<00:10,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45099 speeches for congress 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  86%|████████▌ | 31/36 [00:54<00:08,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44499 speeches for congress 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  89%|████████▉ | 32/36 [00:56<00:06,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49258 speeches for congress 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  92%|█████████▏| 33/36 [00:57<00:04,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44920 speeches for congress 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  94%|█████████▍| 34/36 [00:58<00:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34665 speeches for congress 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data:  97%|█████████▋| 35/36 [01:00<00:01,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30332 speeches for congress 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading congress data: 100%|██████████| 36/36 [01:01<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27075 speeches for congress 114\n",
      "Sampling speeches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing speeches...\n",
      "Analyzing speeches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing congresses:   0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches in congress 79: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [07:42<00:00,  2.16it/s]\n",
      "Processing congresses:   3%|▎         | 1/36 [07:42<4:29:51, 462.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 79 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 80: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [07:57<00:00,  2.09it/s]\n",
      "Processing congresses:   6%|▌         | 2/36 [15:40<4:27:11, 471.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 80 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 81: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:01<00:00,  2.08it/s]\n",
      "Processing congresses:   8%|▊         | 3/36 [23:42<4:21:54, 476.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 81 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 82: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:05<00:00,  2.06it/s]\n",
      "Processing congresses:  11%|█         | 4/36 [31:47<4:15:58, 479.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 82 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 83: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:14<00:00,  2.02it/s]\n",
      "Processing congresses:  14%|█▍        | 5/36 [40:02<4:10:44, 485.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 83 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 84: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:01<00:00,  2.08it/s]\n",
      "Processing congresses:  17%|█▋        | 6/36 [48:04<4:02:03, 484.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 84 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 85: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:32<00:00,  1.95it/s]\n",
      "Processing congresses:  19%|█▉        | 7/36 [56:37<3:58:29, 493.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 85 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 86: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [08:54<00:00,  1.87it/s]\n",
      "Processing congresses:  22%|██▏       | 8/36 [1:05:31<3:56:19, 506.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 86 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 87: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:27<00:00,  1.76it/s]\n",
      "Processing congresses:  25%|██▌       | 9/36 [1:14:59<3:56:31, 525.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 87 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 88: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:21<00:00,  1.78it/s]\n",
      "Processing congresses:  28%|██▊       | 10/36 [1:24:20<3:52:34, 536.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 88 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 89: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:32<00:00,  1.75it/s]\n",
      "Processing congresses:  31%|███       | 11/36 [1:33:53<3:48:12, 547.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 89 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 90: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:34<00:00,  1.74it/s]\n",
      "Processing congresses:  33%|███▎      | 12/36 [1:43:27<3:42:18, 555.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 90 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 91: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:33<00:00,  1.74it/s]\n",
      "Processing congresses:  36%|███▌      | 13/36 [1:53:00<3:35:04, 561.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 91 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 92: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:29<00:00,  1.76it/s]\n",
      "Processing congresses:  39%|███▉      | 14/36 [2:02:30<3:26:39, 563.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 92 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 93: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:33<00:00,  1.74it/s]\n",
      "Processing congresses:  42%|████▏     | 15/36 [2:12:04<3:18:21, 566.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 93 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 94: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:34<00:00,  1.74it/s]\n",
      "Processing congresses:  44%|████▍     | 16/36 [2:21:38<3:09:39, 568.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 94 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 95: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:29<00:00,  1.76it/s]\n",
      "Processing congresses:  47%|████▋     | 17/36 [2:31:07<3:00:12, 569.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 95 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 96: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "Processing congresses:  50%|█████     | 18/36 [2:40:27<2:49:52, 566.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 96 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 97: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:07<00:00,  1.83it/s]\n",
      "Processing congresses:  53%|█████▎    | 19/36 [2:49:35<2:38:51, 560.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 97 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 98: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "Processing congresses:  56%|█████▌    | 20/36 [2:58:54<2:29:26, 560.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 98 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 99: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:15<00:00,  1.80it/s]\n",
      "Processing congresses:  58%|█████▊    | 21/36 [3:08:10<2:19:46, 559.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 99 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 100: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:03<00:00,  1.84it/s]\n",
      "Processing congresses:  61%|██████    | 22/36 [3:17:14<2:09:22, 554.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 100 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 101: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:03<00:00,  1.84it/s]\n",
      "Processing congresses:  64%|██████▍   | 23/36 [3:26:17<1:59:23, 551.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 101 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 102: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:07<00:00,  1.83it/s]\n",
      "Processing congresses:  67%|██████▋   | 24/36 [3:35:25<1:50:00, 550.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 102 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 103: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "Processing congresses:  69%|██████▉   | 25/36 [3:44:44<1:41:21, 552.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 103 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 104: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:20<00:00,  1.78it/s]\n",
      "Processing congresses:  72%|███████▏  | 26/36 [3:54:05<1:32:32, 555.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 104 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 105: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:27<00:00,  1.76it/s]\n",
      "Processing congresses:  75%|███████▌  | 27/36 [4:03:33<1:23:50, 558.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 105 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 106: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:23<00:00,  1.77it/s]\n",
      "Processing congresses:  78%|███████▊  | 28/36 [4:12:57<1:14:43, 560.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 106 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 107: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:28<00:00,  1.76it/s]\n",
      "Processing congresses:  81%|████████  | 29/36 [4:22:25<1:05:39, 562.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 107 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 108: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:27<00:00,  1.76it/s]\n",
      "Processing congresses:  83%|████████▎ | 30/36 [4:31:53<56:25, 564.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 108 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 109: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:44<00:00,  1.71it/s]\n",
      "Processing congresses:  86%|████████▌ | 31/36 [4:41:37<47:31, 570.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 109 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 110: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:36<00:00,  1.73it/s]\n",
      "Processing congresses:  89%|████████▉ | 32/36 [4:51:14<38:09, 572.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 110 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 111: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:35<00:00,  1.74it/s]\n",
      "Processing congresses:  92%|█████████▏| 33/36 [5:00:50<28:39, 573.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 111 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 112: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:39<00:00,  1.73it/s]\n",
      "Processing congresses:  94%|█████████▍| 34/36 [5:10:29<19:10, 575.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 112 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 113: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:31<00:00,  1.75it/s]\n",
      "Processing congresses:  97%|█████████▋| 35/36 [5:20:01<09:34, 574.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 113 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "Number of speeches in congress 114: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing speeches: 100%|██████████| 1000/1000 [09:37<00:00,  1.73it/s]\n",
      "Processing congresses: 100%|██████████| 36/36 [5:29:38<00:00, 549.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Congress 114 results:\n",
      "Number of speeches processed: 1000\n",
      "Columns: ['congress', 'speech_id', 'party', 'issues', 'emotional_intensity', 'political_spectrum', 'emotional_confidence', 'political_confidence']\n",
      "Party distribution: party\n",
      "D    500\n",
      "R    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Analyzing framing shifts...\n",
      "Analyzing framing shifts...\n",
      "\n",
      "Calculating polarization metrics...\n",
      "Calculating polarization metrics...\n",
      "\n",
      "Generating visualization plots...\n",
      "\n",
      "Saving analysis results...\n",
      "\n",
      "Generating summary report...\n",
      "\n",
      "Analysis complete! Results saved to 'analysis_results' directory.\n",
      "\n",
      "Generated files:\n",
      "1. analysis_results/plots/ - Visualization plots\n",
      "2. analysis_results/trend_data.json - Raw trend data\n",
      "3. analysis_results/summary_report.txt - Analysis summary\n",
      "\n",
      "Saving complete dataset...\n",
      "Complete dataset saved successfully!\n",
      "Files created in analysis_results:\n",
      "1. raw_speech_data.json - Complete speech-level analysis results\n",
      "2. trend_data.json - Aggregated trends by party and issue\n",
      "3. polarization_data.json - Polarization metrics over time\n",
      "4. metadata.json - Data format documentation and mappings\n",
      "\n",
      "Saving extended dataset...\n",
      "Extended dataset saved successfully!\n",
      "Additional files created in analysis_results:\n",
      "1. extended_metrics.json - Detailed metrics and volatility measures\n",
      "2. issue_dynamics.json - Issue-specific dynamics and trends\n",
      "3. run_configuration.json - Run parameters and configuration\n",
      "\n",
      "Saved raw trends data to raw_trends.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    base_paths = {\n",
    "        'bound': \"../hein-bound\",  # Path to bound speeches (79-111)\n",
    "        'daily': \"../hein-daily\"   # Path to daily speeches (112-114)\n",
    "    }\n",
    "    \n",
    "    # Model paths\n",
    "    issue_model_path = \"../issue_classifier_eval/model/saved_issue_model\"  # Path to saved issue classification model\n",
    "    axis_model_path = \"../large-training-output/model_artifacts_20241202_142615/model.pt\"  # Path to saved axis prediction model\n",
    "    \n",
    "    # Initialize analysis\n",
    "    print(\"Initializing analysis pipeline...\")\n",
    "    analyzer = CongressionalAnalysis(\n",
    "        issue_model_path=issue_model_path,\n",
    "        axis_model_path=axis_model_path,\n",
    "        congress_range=range(79, 115)\n",
    "    )\n",
    "    \n",
    "    # Load and process data\n",
    "    print(\"\\nLoading congressional data...\")\n",
    "    analyzer.load_data(\n",
    "        base_paths=base_paths,\n",
    "        sample_size=1000\n",
    "    )\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"\\nAnalyzing speeches...\")\n",
    "    analyzer.analyze_speeches()\n",
    "    \n",
    "    # Analyze framing shifts\n",
    "    print(\"\\nAnalyzing framing shifts...\")\n",
    "    analyzer.analyze_framing_shifts()\n",
    "    \n",
    "    # Calculate polarization metrics\n",
    "    print(\"\\nCalculating polarization metrics...\")\n",
    "    analyzer.calculate_polarization_metrics()\n",
    "    \n",
    "    # Generate plots\n",
    "    print(\"\\nGenerating visualization plots...\")\n",
    "    analyzer.plot_framing_trends(save_dir='analysis_results/plots')\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nSaving analysis results...\")\n",
    "    results_dir = 'analysis_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save trend data\n",
    "    trend_data = {\n",
    "        'by_party': {\n",
    "            party: df.to_dict('records')\n",
    "            for party, df in analyzer.trend_dfs['by_party'].items()\n",
    "        },\n",
    "        'by_issue': {\n",
    "            analyzer.ISSUE_MAP[issue]: df.to_dict('records') # issue_name = analyzer.ISSUE_MAP[issue]\n",
    "            for issue, df in analyzer.trend_dfs['by_issue'].items()\n",
    "        },\n",
    "        'polarization': {\n",
    "            (analyzer.ISSUE_MAP[issue] if issue in analyzer.ISSUE_MAP else issue): df.to_dict('records')\n",
    "            for issue, df in analyzer.polarization_dfs.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/trend_data.json\", 'w') as f:\n",
    "        json.dump(trend_data, f, indent=2)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\nGenerating summary report...\")\n",
    "    with open(f\"{results_dir}/summary_report.txt\", 'w') as f:\n",
    "        f.write(\"Congressional Speech Analysis Summary\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        \n",
    "        f.write(\"Analysis Parameters:\\n\")\n",
    "        f.write(f\"- Congress Range: 79-114\\n\")\n",
    "        f.write(f\"- Speeches per Congress: 1000\\n\")\n",
    "        f.write(f\"- Total Speeches Analyzed: {sum(len(df) for df in analyzer.analysis_results.values())}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Overall Trends:\\n\")\n",
    "        f.write(\"--------------\\n\")\n",
    "        for party in ['D', 'R']:\n",
    "            party_name = 'Democratic' if party == 'D' else 'Republican'\n",
    "            df = analyzer.trend_dfs['by_party'][party]\n",
    "            \n",
    "            f.write(f\"\\n{party_name} Party:\\n\")\n",
    "            f.write(f\"- Emotional Intensity Change: {df['emotional_avg'].iloc[-1] - df['emotional_avg'].iloc[0]:.2f}\\n\")\n",
    "            f.write(f\"- Political Position Change: {df['political_avg'].iloc[-1] - df['political_avg'].iloc[0]:.2f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nPolarization Analysis:\\n\")\n",
    "        f.write(\"---------------------\\n\")\n",
    "        df = analyzer.polarization_dfs['overall']\n",
    "        f.write(f\"- Initial Emotional Gap: {df['emotional_gap'].iloc[0]:.2f}\\n\")\n",
    "        f.write(f\"- Final Emotional Gap: {df['emotional_gap'].iloc[-1]:.2f}\\n\")\n",
    "        f.write(f\"- Initial Political Gap: {df['political_gap'].iloc[0]:.2f}\\n\")\n",
    "        f.write(f\"- Final Political Gap: {df['political_gap'].iloc[-1]:.2f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nIssue-Specific Findings:\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        for issue in analyzer.unique_issues:\n",
    "            if issue in analyzer.polarization_dfs:\n",
    "                issue_name = analyzer.ISSUE_MAP[issue]\n",
    "                df = analyzer.polarization_dfs[issue]\n",
    "                f.write(f\"\\n{issue_name}:\\n\")\n",
    "                f.write(f\"- Polarization Change: {df['political_gap'].iloc[-1] - df['political_gap'].iloc[0]:.2f}\\n\")\n",
    "                f.write(f\"- Emotional Intensity Change: {df['emotional_gap'].iloc[-1] - df['emotional_gap'].iloc[0]:.2f}\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'analysis_results' directory.\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"1. analysis_results/plots/ - Visualization plots\")\n",
    "    print(\"2. analysis_results/trend_data.json - Raw trend data\")\n",
    "    print(\"3. analysis_results/summary_report.txt - Analysis summary\")\n",
    "\n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_analyzer = main()\n",
    "    output_analyzer.save_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
