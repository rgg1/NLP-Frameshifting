{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File for doing our axis classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some unlabeled data by removing axis labels from gpt output, so model has labeled and unlabeled data\n",
    "\n",
    "with open('outputs/speeches_114_trimmed_gpt_axis_labels.json', 'r') as f:\n",
    "    speeches = json.load(f)\n",
    "\n",
    "# Remove the specified keys from each speech entry\n",
    "for speech_key in speeches.keys():\n",
    "    speeches[speech_key].pop('emotional_intensity', None)\n",
    "    speeches[speech_key].pop('political_spectrum', None)\n",
    "\n",
    "# Write the updated data to the output file\n",
    "with open('outputs/speeches_114_trimmed_unlabeled_axis.json', 'w') as f:\n",
    "    json.dump(speeches, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, speeches, labels=None, tokenizer=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset for political speeches\n",
    "        \n",
    "        Args:\n",
    "            speeches (dict): Dictionary of speech IDs to speech text\n",
    "            labels (dict): Dictionary of speech IDs to labels (optional)\n",
    "            tokenizer: RoBERTa tokenizer\n",
    "            max_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.speeches = speeches\n",
    "        self.speech_ids = list(speeches.keys())\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer or RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_id = self.speech_ids[idx]\n",
    "        speech = self.speeches[speech_id]['speech']\n",
    "\n",
    "        # Tokenize speech\n",
    "        encoding = self.tokenizer(\n",
    "            speech,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'speech_id': speech_id\n",
    "        }\n",
    "\n",
    "        # Add labels if available\n",
    "        if self.labels is not None:\n",
    "            item['emotional_intensity'] = torch.tensor(self.labels[speech_id]['emotional_intensity'] - 1)  # 0-based indexing\n",
    "            item['political_spectrum'] = torch.tensor(self.labels[speech_id]['political_spectrum'] - 1)\n",
    "\n",
    "        return item\n",
    "\n",
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        # Freeze some layers to prevent overfitting\n",
    "        for param in self.roberta.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classifier heads\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        emotional_logits = self.emotional_classifier(pooled_output)\n",
    "        political_logits = self.political_classifier(pooled_output)\n",
    "\n",
    "        return emotional_logits, political_logits\n",
    "\n",
    "def load_data(labeled_file, unlabeled_file=None):\n",
    "    \"\"\"Load labeled and unlabeled data from JSON files\"\"\"\n",
    "    with open(labeled_file, 'r') as f:\n",
    "        labeled_data = json.load(f)\n",
    "    \n",
    "    unlabeled_data = None\n",
    "    if unlabeled_file:\n",
    "        with open(unlabeled_file, 'r') as f:\n",
    "            unlabeled_data = json.load(f)\n",
    "    \n",
    "    return labeled_data, unlabeled_data\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=5, device='cuda'):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            emotional_labels = batch['emotional_intensity'].to(device)\n",
    "            political_labels = batch['political_spectrum'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(emotional_logits, emotional_labels) + criterion(political_logits, political_labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_emotional = 0\n",
    "        correct_political = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                emotional_labels = batch['emotional_intensity'].to(device)\n",
    "                political_labels = batch['political_spectrum'].to(device)\n",
    "                \n",
    "                emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "                \n",
    "                loss = criterion(emotional_logits, emotional_labels) + criterion(political_logits, political_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, emotional_predicted = torch.max(emotional_logits, 1)\n",
    "                _, political_predicted = torch.max(political_logits, 1)\n",
    "                \n",
    "                correct_emotional += (emotional_predicted == emotional_labels).sum().item()\n",
    "                correct_political += (political_predicted == political_labels).sum().item()\n",
    "                total += emotional_labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        emotional_accuracy = 100 * correct_emotional / total\n",
    "        political_accuracy = 100 * correct_political / total\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Emotional Accuracy: {emotional_accuracy:.2f}%')\n",
    "        print(f'Political Accuracy: {political_accuracy:.2f}%')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def predict(model, dataset, device='cuda'):\n",
    "    \"\"\"Generate predictions for unlabeled data\"\"\"\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Generating predictions'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            speech_ids = batch['speech_id']\n",
    "            \n",
    "            emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            emotional_preds = torch.max(emotional_logits, 1)[1]\n",
    "            political_preds = torch.max(political_logits, 1)[1]\n",
    "            \n",
    "            # Convert to 1-based indexing and move to CPU\n",
    "            emotional_preds = emotional_preds.cpu().numpy() + 1\n",
    "            political_preds = political_preds.cpu().numpy() + 1\n",
    "            \n",
    "            for speech_id, emotional, political in zip(speech_ids, emotional_preds, political_preds):\n",
    "                predictions[speech_id] = {\n",
    "                    'emotional_intensity': int(emotional),\n",
    "                    'political_spectrum': int(political)\n",
    "                }\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 5/5 [02:55<00:00, 35.09s/it, loss=4.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.4675\n",
      "Emotional Accuracy: 15.00%\n",
      "Political Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5/5 [02:38<00:00, 31.77s/it, loss=4.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.2086\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5/5 [02:26<00:00, 29.29s/it, loss=3.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.1003\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5/5 [02:13<00:00, 26.73s/it, loss=3.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.0470\n",
      "Emotional Accuracy: 25.00%\n",
      "Political Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5/5 [02:22<00:00, 28.49s/it, loss=3.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.0293\n",
      "Emotional Accuracy: 25.00%\n",
      "Political Accuracy: 45.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 6/6 [00:17<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data (labeled, unlabeled)\n",
    "    labeled_data, unlabeled_data = load_data('outputs/speeches_113_trimmed_gpt_axis_labels.json', 'outputs/speeches_114_trimmed_unlabeled_axis.json')\n",
    "    \n",
    "    # Split labeled data into train/val\n",
    "    train_ids, val_ids = train_test_split(list(labeled_data.keys()), test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_data = {k: labeled_data[k] for k in train_ids}\n",
    "    val_data = {k: labeled_data[k] for k in val_ids}\n",
    "    \n",
    "    # Create datasets\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    train_dataset = SpeechDataset(train_data, train_data, tokenizer)\n",
    "    val_dataset = SpeechDataset(val_data, val_data, tokenizer)\n",
    "    \n",
    "    if unlabeled_data:\n",
    "        predict_dataset = SpeechDataset(unlabeled_data, tokenizer=tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PoliticalSpeechClassifier()\n",
    "    \n",
    "    # Train model\n",
    "    best_model_state = train_model(model, train_loader, val_loader, device=device)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Generate predictions for unlabeled data\n",
    "    if unlabeled_data:\n",
    "        predictions = predict(model, predict_dataset, device)\n",
    "        \n",
    "        # Save predictions\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(predictions, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
