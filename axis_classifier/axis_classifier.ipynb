{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File for doing our axis classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing speech files\n",
    "data_dir = '.'\n",
    "\n",
    "# Get list of files in directory, filtering out directories\n",
    "files = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]\n",
    "\n",
    "# Initialize lists to store data\n",
    "speech_counts = []\n",
    "emotional_intensity = []\n",
    "political_spectrum = []\n",
    "\n",
    "# Iterate over files\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Get speech count\n",
    "    speech_counts.append(len(data))\n",
    "\n",
    "    # Extract ratings\n",
    "    emotional_intensity.extend([value['emotional_intensity'] for value in data.values()])\n",
    "    political_spectrum.extend([value['political_spectrum'] for value in data.values()])\n",
    "\n",
    "data = {\n",
    "    'File': files,\n",
    "    'Speech Count': speech_counts\n",
    "}\n",
    "\n",
    "# Display total distribution of ratings\n",
    "total_distribution = {\n",
    "    'Emotional Intensity': pd.Series(emotional_intensity).value_counts(normalize=True),\n",
    "    'Political Spectrum': pd.Series(political_spectrum).value_counts(normalize=True)\n",
    "}\n",
    "\n",
    "print('Total Distribution of Ratings:')\n",
    "print(pd.DataFrame(total_distribution))\n",
    "\n",
    "# make sure we sort by file name so we get files in increasing order (they contain numbers)\n",
    "print('\\nTotal number of speeches by File:')\n",
    "df = pd.DataFrame(data).sort_values('File')\n",
    "print(df)\n",
    "\n",
    "# get the total number of speeches across all files\n",
    "total_speeches = df['Speech Count'].sum()\n",
    "print(f'\\nTotal number of speeches: {total_speeches}')\n",
    "\n",
    "# now, get the distribution of ratings by file\n",
    "for file in files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    emotional_intensity = [value['emotional_intensity'] for value in data.values()]\n",
    "    political_spectrum = [value['political_spectrum'] for value in data.values()]\n",
    "\n",
    "    distribution = {\n",
    "        'Emotional Intensity': pd.Series(emotional_intensity).value_counts(normalize=True),\n",
    "        'Political Spectrum': pd.Series(political_spectrum).value_counts(normalize=True)\n",
    "    }\n",
    "\n",
    "    print(f'\\nDistribution of Ratings for {file}:')\n",
    "    print(pd.DataFrame(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(file_pattern='speeches_*_gpt_axis_labels.json'):\n",
    "    \"\"\"\n",
    "    Load and combine all data files\n",
    "\n",
    "    Args:\n",
    "        file_pattern (str): File pattern to match\n",
    "\n",
    "    Returns:\n",
    "        dict: Combined data\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    for file_path in glob.glob(file_pattern):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_data.update(data)\n",
    "    return all_data\n",
    "\n",
    "def calculate_class_weights(data):\n",
    "    \"\"\"\n",
    "    Calculate class weights for both tasks\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data dictionary\n",
    "\n",
    "    Returns:\n",
    "        dict: Emotional intensity and political spectrum weights\n",
    "    \"\"\"\n",
    "    emotional_counts = Counter(item['emotional_intensity'] for item in data.values())\n",
    "    political_counts = Counter(item['political_spectrum'] for item in data.values())\n",
    "\n",
    "    total = len(data)\n",
    "\n",
    "    emotional_weights = {\n",
    "        label: total / (count * 5)  # 5 = number of classes\n",
    "        for label, count in emotional_counts.items()\n",
    "    }\n",
    "\n",
    "    political_weights = {\n",
    "        label: total / (count * 5)\n",
    "        for label, count in political_counts.items()\n",
    "    }\n",
    "\n",
    "    return emotional_weights, political_weights\n",
    "\n",
    "def create_balanced_sampler(data):\n",
    "    \"\"\"\n",
    "    Create a weighted sampler to balance the dataset\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data dictionary\n",
    "\n",
    "    Returns:\n",
    "        WeightedRandomSampler: Weight\n",
    "    \"\"\"\n",
    "    # Get counts for both tasks\n",
    "    emotional_labels = [item['emotional_intensity'] for item in data.values()]\n",
    "    political_labels = [item['political_spectrum'] for item in data.values()]\n",
    "\n",
    "    emotional_counts = Counter(emotional_labels)\n",
    "    political_counts = Counter(political_labels)\n",
    "\n",
    "    # Calculate weights for each sample\n",
    "    weights = []\n",
    "    for id in data.keys():\n",
    "        e_label = data[id]['emotional_intensity']\n",
    "        p_label = data[id]['political_spectrum']\n",
    "\n",
    "        # Combined weight is product of individual weights\n",
    "        e_weight = 1 / emotional_counts[e_label]\n",
    "        p_weight = 1 / political_counts[p_label]\n",
    "        weight = np.sqrt(e_weight * p_weight)  # mean of weights\n",
    "        weights.append(weight)\n",
    "\n",
    "    return WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, speeches, tokenizer=None, max_length=512):\n",
    "        self.speeches = speeches\n",
    "        self.speech_ids = list(speeches.keys())\n",
    "        self.tokenizer = tokenizer or RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speech_id = self.speech_ids[idx]\n",
    "        speech_data = self.speeches[speech_id]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            speech_data['speech'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'emotional_intensity': torch.tensor(speech_data['emotional_intensity'] - 1),\n",
    "            'political_spectrum': torch.tensor(speech_data['political_spectrum'] - 1),\n",
    "            'speech_id': speech_id\n",
    "        }\n",
    "\n",
    "        return item\n",
    "\n",
    "def plot_class_distributions(data, save_path='distribution_plots.png'):\n",
    "    \"\"\"\n",
    "    Plot distribution of classes for both tasks\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data dictionary\n",
    "        save_path (str): Path to save the plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    emotional_labels = [item['emotional_intensity'] for item in data.values()]\n",
    "    political_labels = [item['political_spectrum'] for item in data.values()]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Emotional Intensity Distribution\n",
    "    sns.countplot(x=emotional_labels, ax=ax1)\n",
    "    ax1.set_title('Emotional Intensity Distribution')\n",
    "    ax1.set_xlabel('Intensity Level')\n",
    "    ax1.set_ylabel('Count')\n",
    "\n",
    "    # Political Spectrum Distribution\n",
    "    sns.countplot(x=political_labels, ax=ax2)\n",
    "    ax2.set_title('Political Spectrum Distribution')\n",
    "    ax2.set_xlabel('Political Position')\n",
    "    ax2.set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrices(true_emotional, pred_emotional, true_political, pred_political, save_path='confusion_matrices.png'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for both tasks\n",
    "    \n",
    "    Args:\n",
    "        true_emotional (list): True emotional intensity labels\n",
    "        pred_emotional (list): Predicted emotional intensity labels\n",
    "        true_political (list): True political spectrum labels\n",
    "        pred_political (list): Predicted political spectrum labels\n",
    "        save_path (str): Path to save the plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Emotional Intensity Confusion Matrix\n",
    "    cm_emotional = confusion_matrix(true_emotional, pred_emotional)\n",
    "    sns.heatmap(cm_emotional, annot=True, fmt='d', ax=ax1)\n",
    "    ax1.set_title('Emotional Intensity Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('True')\n",
    "\n",
    "    # Political Spectrum Confusion Matrix\n",
    "    cm_political = confusion_matrix(true_political, pred_political)\n",
    "    sns.heatmap(cm_political, annot=True, fmt='d', ax=ax2)\n",
    "    ax2.set_title('Political Spectrum Confusion Matrix')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliticalSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # Unfreeze more layers since we have more data\n",
    "        for param in self.roberta.encoder.layer[-8:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Shared features layer\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Task-specific layers\n",
    "        self.emotional_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.political_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use mean pooling instead of just [CLS] token\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_expanded.sum(1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "\n",
    "        # Get shared features\n",
    "        shared_features = self.shared_features(pooled_output)\n",
    "\n",
    "        # Get task-specific predictions\n",
    "        emotional_logits = self.emotional_classifier(shared_features)\n",
    "        political_logits = self.political_classifier(shared_features)\n",
    "\n",
    "        return emotional_logits, political_logits\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.emotional_accuracies = []\n",
    "        self.political_accuracies = []\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_emotional_acc = 0\n",
    "        self.best_political_acc = 0\n",
    "\n",
    "    def update(self, train_loss, val_loss, emotional_acc, political_acc):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.emotional_accuracies.append(emotional_acc)\n",
    "        self.political_accuracies.append(political_acc)\n",
    "\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "        if emotional_acc > self.best_emotional_acc:\n",
    "            self.best_emotional_acc = emotional_acc\n",
    "        if political_acc > self.best_political_acc:\n",
    "            self.best_political_acc = political_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize focal loss for both\n",
    "    emotional_criterion = FocalLoss(gamma=2)\n",
    "    political_criterion = FocalLoss(gamma=2)\n",
    "\n",
    "    # Optimizer with different learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if 'roberta' in n],\n",
    "            'lr': 1e-5\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if 'roberta' not in n],\n",
    "            'lr': 3e-4\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler with warmup\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    metric_tracker = MetricTracker()\n",
    "    best_model_state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'epoch': 0,\n",
    "        'val_loss': float('inf'),\n",
    "        'emotional_accuracy': 0,\n",
    "        'political_accuracy': 0,\n",
    "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_emotional_correct = 0\n",
    "        train_political_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            emotional_labels = batch['emotional_intensity'].to(device)\n",
    "            political_labels = batch['political_spectrum'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate losses\n",
    "            emotional_loss = emotional_criterion(emotional_logits, emotional_labels)\n",
    "            political_loss = political_criterion(political_logits, political_labels)\n",
    "            total_loss = emotional_loss + political_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_train_loss += total_loss.item()\n",
    "\n",
    "            _, emotional_preds = torch.max(emotional_logits, 1)\n",
    "            _, political_preds = torch.max(political_logits, 1)\n",
    "\n",
    "            train_emotional_correct += (emotional_preds == emotional_labels).sum().item()\n",
    "            train_political_correct += (political_preds == political_labels).sum().item()\n",
    "            train_total += emotional_labels.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss.item(),\n",
    "                'e_acc': 100 * train_emotional_correct / train_total,\n",
    "                'p_acc': 100 * train_political_correct / train_total\n",
    "            })\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_emotional_correct = 0\n",
    "        val_political_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        val_emotional_preds = []\n",
    "        val_emotional_true = []\n",
    "        val_political_preds = []\n",
    "        val_political_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                emotional_labels = batch['emotional_intensity'].to(device)\n",
    "                political_labels = batch['political_spectrum'].to(device)\n",
    "\n",
    "                emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                emotional_loss = emotional_criterion(emotional_logits, emotional_labels)\n",
    "                political_loss = political_criterion(political_logits, political_labels)\n",
    "                val_loss = emotional_loss + political_loss\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, emotional_preds = torch.max(emotional_logits, 1)\n",
    "                _, political_preds = torch.max(political_logits, 1)\n",
    "\n",
    "                val_emotional_correct += (emotional_preds == emotional_labels).sum().item()\n",
    "                val_political_correct += (political_preds == political_labels).sum().item()\n",
    "                val_total += emotional_labels.size(0)\n",
    "\n",
    "                # Store predictions and true labels for confusion matrix\n",
    "                val_emotional_preds.extend(emotional_preds.cpu().numpy())\n",
    "                val_emotional_true.extend(emotional_labels.cpu().numpy())\n",
    "                val_political_preds.extend(political_preds.cpu().numpy())\n",
    "                val_political_true.extend(political_labels.cpu().numpy())\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        emotional_accuracy = 100 * val_emotional_correct / val_total\n",
    "        political_accuracy = 100 * val_political_correct / val_total\n",
    "\n",
    "        # Update metric tracker\n",
    "        metric_tracker.update(avg_train_loss, avg_val_loss, emotional_accuracy, political_accuracy)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < metric_tracker.best_val_loss:\n",
    "            best_model_state = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'emotional_accuracy': emotional_accuracy,\n",
    "                'political_accuracy': political_accuracy,\n",
    "                'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            }\n",
    "\n",
    "        # Save checkpoint every 3 epochs and at the end\n",
    "        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:\n",
    "            checkpoint_dir = save_checkpoint(\n",
    "                best_model_state,\n",
    "                metric_tracker,\n",
    "                epoch + 1\n",
    "            )\n",
    "            print(f\"\\nCheckpoint saved at: {checkpoint_dir}\")\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average Val Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Emotional Accuracy: {emotional_accuracy:.2f}%')\n",
    "        print(f'Political Accuracy: {political_accuracy:.2f}%')\n",
    "\n",
    "        # Plot confusion matrices for this epoch\n",
    "        plot_confusion_matrices(\n",
    "            val_emotional_true, val_emotional_preds,\n",
    "            val_political_true, val_political_preds,\n",
    "            save_path=f'confusion_matrices_epoch_{epoch+1}.png'\n",
    "        )\n",
    "\n",
    "    return best_model_state, metric_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|██████████| 5/5 [02:18<00:00, 27.76s/it, loss=3.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1975\n",
      "Emotional Accuracy: 15.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5/5 [02:09<00:00, 25.85s/it, loss=3.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1929\n",
      "Emotional Accuracy: 15.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5/5 [02:13<00:00, 26.69s/it, loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1831\n",
      "Emotional Accuracy: 15.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5/5 [02:16<00:00, 27.39s/it, loss=3.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1669\n",
      "Emotional Accuracy: 15.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5/5 [02:13<00:00, 26.65s/it, loss=3.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1351\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5/5 [02:05<00:00, 25.07s/it, loss=3.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.0512\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5/5 [02:09<00:00, 26.00s/it, loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.9830\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5/5 [02:05<00:00, 25.10s/it, loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.9271\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5/5 [02:12<00:00, 26.54s/it, loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8738\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5/5 [02:05<00:00, 25.14s/it, loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8214\n",
      "Emotional Accuracy: 30.00%\n",
      "Political Accuracy: 55.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 6/6 [00:17<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model_state, metric_tracker, epoch, base_path='model_checkpoints'):\n",
    "    \"\"\"\n",
    "    Save model checkpoint and metrics\n",
    "    \n",
    "    Args:\n",
    "        model_state (dict): Model state dictionary\n",
    "        metric_tracker (MetricTracker): Metric tracker object\n",
    "        epoch (int): Current epoch\n",
    "        base_path (str): Base path to save the checkpoint\n",
    "\n",
    "    Returns:\n",
    "        str: Path to saved checkpoint\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = f\"{base_path}_epoch{epoch}_{timestamp}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model_state, f\"{save_dir}/model.pt\")\n",
    "\n",
    "    # Save metrics history\n",
    "    metrics_dict = {\n",
    "        'train_losses': metric_tracker.train_losses,\n",
    "        'val_losses': metric_tracker.val_losses,\n",
    "        'emotional_accuracies': metric_tracker.emotional_accuracies,\n",
    "        'political_accuracies': metric_tracker.political_accuracies,\n",
    "        'best_val_loss': metric_tracker.best_val_loss,\n",
    "        'best_emotional_acc': metric_tracker.best_emotional_acc,\n",
    "        'best_political_acc': metric_tracker.best_political_acc\n",
    "    }\n",
    "\n",
    "    with open(f\"{save_dir}/metrics.json\", 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "    return save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(metric_tracker, save_path='training_history.png'):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over time\n",
    "    \n",
    "    Args:\n",
    "        metric_tracker (MetricTracker): Metric tracker object\n",
    "        save_path (str): Path to save the plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Plot losses\n",
    "    epochs = range(1, len(metric_tracker.train_losses) + 1)\n",
    "    ax1.plot(epochs, metric_tracker.train_losses, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, metric_tracker.val_losses, 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, metric_tracker.emotional_accuracies, 'g-', label='Emotional Accuracy')\n",
    "    ax2.plot(epochs, metric_tracker.political_accuracies, 'p-', label='Political Accuracy')\n",
    "    ax2.set_title('Validation Accuracies')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_model_artifacts(model_state, metric_tracker, base_path='model_artifacts'):\n",
    "    \"\"\"\n",
    "    Save model and related artifacts\n",
    "    \n",
    "    Args:\n",
    "        model_state (dict): Model state dictionary\n",
    "        metric_tracker (MetricTracker): Metric tracker object\n",
    "        base_path (str): Base path to save the artifacts\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to saved model artifacts\n",
    "    \"\"\"\n",
    "    timestamp = model_state['timestamp']\n",
    "    save_dir = f\"{base_path}_{timestamp}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model_state, f\"{save_dir}/model.pt\")\n",
    "\n",
    "    # Save metrics history\n",
    "    metrics_dict = {\n",
    "        'train_losses': metric_tracker.train_losses,\n",
    "        'val_losses': metric_tracker.val_losses,\n",
    "        'emotional_accuracies': metric_tracker.emotional_accuracies,\n",
    "        'political_accuracies': metric_tracker.political_accuracies,\n",
    "        'best_val_loss': metric_tracker.best_val_loss,\n",
    "        'best_emotional_acc': metric_tracker.best_emotional_acc,\n",
    "        'best_political_acc': metric_tracker.best_political_acc\n",
    "    }\n",
    "\n",
    "    with open(f\"{save_dir}/metrics.json\", 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "    return save_dir\n",
    "\n",
    "def load_model_for_inference(model_path):\n",
    "    \"\"\"\n",
    "    Load a trained model for inference\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to saved model\n",
    "\n",
    "    Returns:\n",
    "        PoliticalSpeechClassifier: Loaded model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_state = torch.load(model_path, map_location=device)\n",
    "\n",
    "    model = PoliticalSpeechClassifier()\n",
    "    model.load_state_dict(model_state['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_speech(model, tokenizer, speech_text):\n",
    "    \"\"\"\n",
    "    Make predictions for a single speech\n",
    "    \n",
    "    Args:\n",
    "        model (PoliticalSpeechClassifier): Trained model\n",
    "        tokenizer (RobertaTokenizer): Tokenizer\n",
    "        speech_text (str): Speech text\n",
    "\n",
    "    Returns:\n",
    "        dict: Predicted emotional intensity and political spectrum of the speech (1-5) along with confidence scores\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        speech_text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotional_logits, political_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        emotional_probs = torch.softmax(emotional_logits, dim=1)\n",
    "        political_probs = torch.softmax(political_logits, dim=1)\n",
    "\n",
    "        emotional_pred = torch.argmax(emotional_probs, dim=1).item() + 1\n",
    "        political_pred = torch.argmax(political_probs, dim=1).item() + 1\n",
    "\n",
    "        emotional_confidence = emotional_probs[0][emotional_pred-1].item()\n",
    "        political_confidence = political_probs[0][political_pred-1].item()\n",
    "\n",
    "    return {\n",
    "        'emotional_intensity': emotional_pred,\n",
    "        'emotional_confidence': emotional_confidence,\n",
    "        'political_spectrum': political_pred,\n",
    "        'political_confidence': political_confidence\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load all data\n",
    "    print(\"Loading data...\")\n",
    "    all_data = load_all_data()\n",
    "    print(f\"Loaded {len(all_data)} speeches\")\n",
    "\n",
    "    # Plot initial class distributions\n",
    "    plot_class_distributions(all_data)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data, val_data = train_test_split(\n",
    "        list(all_data.items()),\n",
    "        test_size=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_data = dict(train_data)\n",
    "    val_data = dict(val_data)\n",
    "\n",
    "    # Create datasets and samplers\n",
    "    print(\"Creating datasets...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    train_dataset = SpeechDataset(train_data, tokenizer=tokenizer)\n",
    "    val_dataset = SpeechDataset(val_data, tokenizer=tokenizer)\n",
    "\n",
    "    # Create sampler for training data\n",
    "    train_sampler = create_balanced_sampler(train_data)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize and train model\n",
    "    print(\"Training model...\")\n",
    "    model = PoliticalSpeechClassifier()\n",
    "    best_model_state, metric_tracker = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(metric_tracker)\n",
    "\n",
    "    # Save model artifacts\n",
    "    save_dir = save_model_artifacts(best_model_state, metric_tracker)\n",
    "    print(f\"Model artifacts saved to: {save_dir}\")\n",
    "\n",
    "    # Example\n",
    "    example_speech = (\n",
    "        \"Mr. Speaker, I rise today to express my strong support for this crucial \"\n",
    "        \"legislation that will help working families across our great nation...\"\n",
    "    )\n",
    "\n",
    "    model = load_model_for_inference(f\"{save_dir}/model.pt\")\n",
    "    predictions = predict_speech(model, tokenizer, example_speech)\n",
    "\n",
    "    print(\"\\nExample Prediction:\")\n",
    "    print(f\"Emotional Intensity: {predictions['emotional_intensity']} \"\n",
    "          f\"(Confidence: {predictions['emotional_confidence']:.2%})\")\n",
    "    print(f\"Political Spectrum: {predictions['political_spectrum']} \"\n",
    "          f\"(Confidence: {predictions['political_confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodri/Desktop/MIT Fall 2024 Classes/6.8611/Project/NLP-Frameshifting/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/8:   2%|▏         | 1/49 [01:18<1:02:58, 78.72s/it, loss=3.39, emotional_acc=3.12, political_acc=21.9]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
