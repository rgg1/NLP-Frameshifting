{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech(BaseModel):\n",
    "    speech_id: str\n",
    "    emotional_intensity: int = Field(ge=1, le=10)  # Ensure value between 1 and 10\n",
    "    political_spectrum: int = Field(ge=1, le=10)   # Ensure value between 1 and 10\n",
    "\n",
    "class Speeches(BaseModel):\n",
    "    speeches: List[Speech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speech_file(file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads a speech file and returns a dictionary mapping speech_ids to speeches.\n",
    "    Only includes speeches with more than 30 words.\n",
    "    \"\"\"\n",
    "    speeches = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip header line\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # Split on pipe character\n",
    "            parts = line.strip().split('|')\n",
    "            if len(parts) == 2:\n",
    "                speech_id, speech = parts\n",
    "                # Only add speech if it has more than 30 words\n",
    "                if len(speech.split()) > 30:\n",
    "                    speeches[speech_id] = speech\n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_speeches(speeches: Dict[str, str], max_chunk_size: int = 20000) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits speeches into chunks while keeping individual speeches intact.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = {}\n",
    "    current_size = 0\n",
    "    \n",
    "    for speech_id, speech in speeches.items():\n",
    "        speech_size = len(speech)\n",
    "        # If adding this speech would exceed max size and we already have speeches,\n",
    "        # start a new chunk\n",
    "        if current_size + speech_size > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = {}\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk[speech_id] = speech\n",
    "        current_size += speech_size\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_speeches(speeches: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's API to analyze emotional intensity and political spectrum of speeches.\n",
    "    \"\"\"\n",
    "    # Prepare the speeches for analysis\n",
    "    speeches_text = \"\\n\\n\".join([f\"Speech ID: {id}\\nContent: {text}\" for id, text in speeches.items()])\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    Analyze each speech and rate it on two scales:\n",
    "\n",
    "                    1. Emotional Intensity (1-5):\n",
    "                    - 1: Neutral and technical; purely factual presentation with minimal personal expression\n",
    "                    - 2: Mild emotional content; professional tone with clear stance and moderate conviction\n",
    "                    - 3: Moderate emotional engagement; balanced but passionate delivery\n",
    "                    - 4: Strong emotional content; powerful rhetoric and clear passion\n",
    "                    - 5: Extremely emotional; intense passion, dramatic language, and strong calls to action\n",
    "\n",
    "                    2. Political Spectrum (1-5):\n",
    "                    - 1: Strongly Progressive (major reforms, significant system change, strong left policies)\n",
    "                    - 2: Moderately Progressive (incremental changes, center-left policies)\n",
    "                    - 3: Centrist (balance of progressive and traditional views)\n",
    "                    - 4: Moderately Conservative (traditional values, center-right policies)\n",
    "                    - 5: Strongly Conservative (emphasis on traditional values, major system preservation)\n",
    "\n",
    "                    Consider factors like:\n",
    "                    - Language and rhetoric used\n",
    "                    - Policy positions expressed\n",
    "                    - Values emphasized\n",
    "                    - Economic and social views\n",
    "                    - Treatment of traditional vs progressive values\n",
    "\n",
    "                    Output should be in JSON format containing a list of objects, each with:\n",
    "                    - speech_id\n",
    "                    - emotional_intensity (integer 1-5)\n",
    "                    - political_spectrum (integer 1-5)\n",
    "\n",
    "                    Be objective and consistent in your ratings. Use the full range of the scale when appropriate - don't hesitate to use any number if it best matches the speech's content.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": speeches_text\n",
    "                }\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\":\"_\",\n",
    "                    \"schema\": Speeches.model_json_schema()\n",
    "                }\n",
    "            },\n",
    "            temperature=0.3,\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech_file(file_path: str, output_dir: str, max_chunks: int=None):\n",
    "    \"\"\"\n",
    "    Process a speech file and save the results.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    # Read speeches\n",
    "    speeches = read_speech_file(file_path)\n",
    "    print(f\"Found {len(speeches)} speeches\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = chunk_speeches(speeches)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Process each chunk and combine results\n",
    "    all_results = {}\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        try:\n",
    "            # Get analysis for speeches in this chunk\n",
    "            analysis_json = analyze_speeches(chunk)\n",
    "            analysis_results = json.loads(analysis_json)\n",
    "            \n",
    "            # Combine speech text with analysis\n",
    "            for speech in analysis_results[\"speeches\"]:\n",
    "                speech_id = speech[\"speech_id\"]\n",
    "                all_results[speech_id] = {\n",
    "                    \"speech\": speeches[speech_id],\n",
    "                    \"emotional_intensity\": speech[\"emotional_intensity\"],\n",
    "                    \"political_spectrum\": speech[\"political_spectrum\"]\n",
    "                }\n",
    "            # print if i mod 10 == 0\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Successfully processed chunk {i+1}\")\n",
    "            \n",
    "            # break after max_chunks\n",
    "            if max_chunks and i == max_chunks:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path).replace('.txt', '_gpt_axis_labels.json'))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech_files(speech_files: List[str], input_dir: str, output_dir: str, max_chunks: int=None):\n",
    "    \"\"\"\n",
    "    Process multiple speech files.\n",
    "    \"\"\"\n",
    "    for file_name in tqdm(speech_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        process_speech_file(file_path, output_dir, max_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(emotional_df: pd.DataFrame, political_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create visualizations for the rating distributions.\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot Emotional Intensity\n",
    "    ax1.bar(emotional_df['Rating'], emotional_df['Percentage'])\n",
    "    ax1.set_title('Distribution of Emotional Intensity Ratings')\n",
    "    ax1.set_xlabel('Emotional Intensity Rating (1-5)')\n",
    "    ax1.set_ylabel('Percentage of Speeches')\n",
    "    ax1.set_xticks(range(1, 6))\n",
    "    \n",
    "    # Plot Political Spectrum\n",
    "    ax2.bar(political_df['Rating'], political_df['Percentage'])\n",
    "    ax2.set_title('Distribution of Political Spectrum Ratings')\n",
    "    ax2.set_xlabel('Political Spectrum Rating (1=Far Left, 5=Far Right)')\n",
    "    ax2.set_ylabel('Percentage of Speeches')\n",
    "    ax2.set_xticks(range(1, 6))\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rating_distributions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(counts: dict, total: int):\n",
    "    \"\"\"\n",
    "    Print the distribution of ratings in a formatted way.\n",
    "    \"\"\"\n",
    "    for rating in range(1, 6):\n",
    "        count = counts.get(rating, 0)\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"Rating {rating}: {count:4d} speeches ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ratings(input_dir: str):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of emotional intensity and political spectrum ratings\n",
    "    from all JSON files in the input directory.\n",
    "    \"\"\"\n",
    "    # Initialize counters for both metrics\n",
    "    emotional_counts = defaultdict(int)\n",
    "    political_counts = defaultdict(int)\n",
    "    total_speeches = 0\n",
    "    \n",
    "    # Process each JSON file in the directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('_gpt_axis_labels.json'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Count frequencies for each rating\n",
    "                for speech_id, speech_data in data.items():\n",
    "                    emotional_counts[speech_data['emotional_intensity']] += 1\n",
    "                    political_counts[speech_data['political_spectrum']] += 1\n",
    "                    total_speeches += 1\n",
    "    \n",
    "    # Convert to pandas DataFrames for easier analysis and visualization\n",
    "    emotional_df = pd.DataFrame([\n",
    "        {'Rating': rating, 'Count': count, 'Percentage': (count/total_speeches)*100}\n",
    "        for rating, count in sorted(emotional_counts.items())\n",
    "    ])\n",
    "    \n",
    "    political_df = pd.DataFrame([\n",
    "        {'Rating': rating, 'Count': count, 'Percentage': (count/total_speeches)*100}\n",
    "        for rating, count in sorted(political_counts.items())\n",
    "    ])\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Total speeches analyzed: {total_speeches}\\n\")\n",
    "    print(\"Emotional Intensity Distribution:\")\n",
    "    print_distribution(emotional_counts, total_speeches)\n",
    "    print(\"\\nPolitical Spectrum Distribution:\")\n",
    "    print_distribution(political_counts, total_speeches)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(emotional_df, political_df)\n",
    "    \n",
    "    return {\n",
    "        'emotional_intensity': dict(emotional_counts),\n",
    "        'political_spectrum': dict(political_counts),\n",
    "        'total_speeches': total_speeches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations(input_dir: str):\n",
    "    \"\"\"\n",
    "    Calculate correlation between emotional intensity and political spectrum ratings.\n",
    "    \"\"\"\n",
    "    ratings_pairs = []\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('_gpt_axis_labels.json'):\n",
    "            with open(os.path.join(input_dir, filename), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for speech_data in data.values():\n",
    "                    ratings_pairs.append({\n",
    "                        'emotional_intensity': speech_data['emotional_intensity'],\n",
    "                        'political_spectrum': speech_data['political_spectrum']\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(ratings_pairs)\n",
    "    correlation = df['emotional_intensity'].corr(df['political_spectrum'])\n",
    "    \n",
    "    print(f\"\\nCorrelation between Emotional Intensity and Political Spectrum: {correlation:.3f}\")\n",
    "    \n",
    "    # Create correlation visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df['political_spectrum'], df['emotional_intensity'], alpha=0.5)\n",
    "    plt.title('Correlation: Emotional Intensity vs Political Spectrum')\n",
    "    plt.xlabel('Political Spectrum Rating (1=Far Left, 5=Far Right)')\n",
    "    plt.ylabel('Emotional Intensity Rating')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('rating_correlation.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../small_speech_data/speeches_113_trimmed.txt\n",
      "Found 6563 speeches\n",
      "Split into 891 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  11%|█         | 100/891 [06:03<47:53,  3.63s/it]\n",
      "Processing files:  50%|█████     | 1/2 [06:03<06:03, 363.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 101\n",
      "\n",
      "Results saved to outputs/speeches_113_trimmed_gpt_axis_labels.json\n",
      "Processing file: ../small_speech_data/speeches_114_trimmed.txt\n",
      "Found 5456 speeches\n",
      "Split into 668 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▍        | 100/668 [05:41<32:20,  3.42s/it]\n",
      "Processing files: 100%|██████████| 2/2 [11:45<00:00, 352.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 101\n",
      "\n",
      "Results saved to outputs/speeches_114_trimmed_gpt_axis_labels.json\n",
      "Total speeches analyzed: 1812\n",
      "\n",
      "Emotional Intensity Distribution:\n",
      "Rating 1:  444 speeches ( 24.5%)\n",
      "Rating 2:  289 speeches ( 15.9%)\n",
      "Rating 3:  483 speeches ( 26.7%)\n",
      "Rating 4:  525 speeches ( 29.0%)\n",
      "Rating 5:   71 speeches (  3.9%)\n",
      "\n",
      "Political Spectrum Distribution:\n",
      "Rating 1:  176 speeches (  9.7%)\n",
      "Rating 2:  402 speeches ( 22.2%)\n",
      "Rating 3:  947 speeches ( 52.3%)\n",
      "Rating 4:  186 speeches ( 10.3%)\n",
      "Rating 5:  101 speeches (  5.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation between Emotional Intensity and Political Spectrum: -0.220\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"../small_speech_data\"\n",
    "    output_dir = \"outputs\"\n",
    "    speech_files = [f for f in os.listdir(input_dir) if f.startswith(\"speeches_\") and f.endswith(\".txt\")]\n",
    "    process_speech_files(speech_files, input_dir, output_dir, max_chunks=100) # doing 100 max_chunks instead of 10 now\n",
    "\n",
    "    # axis gpt output analysis:\n",
    "\n",
    "    input_dir = \"outputs\"  # Directory containing the analysis JSON files\n",
    "    \n",
    "    # Analyze distributions\n",
    "    results = analyze_ratings(input_dir)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    calculate_correlations(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
